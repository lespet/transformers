{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lespet/transformers/blob/master/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa032160",
      "metadata": {
        "id": "fa032160"
      },
      "source": [
        "# In this notebook, I'll try to apply TRANSFORMER model for a CHATBOT from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698a1320",
      "metadata": {
        "id": "698a1320"
      },
      "source": [
        "## Overview\n",
        "***\n",
        "*A chatbot or chatterbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent chatbot is a type of software that can help human by automating conversations and interact with them through messaging platforms. here are different approaches and tools that you can use when building chatbots. Depending on the use case you want to address, some technologies are more appropriate than others. Combining artificial intelligence forms such as natural language processing, machine learning, and semantic understanding may be the best option to achieve the desired results.*\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94feefe",
      "metadata": {
        "id": "b94feefe"
      },
      "source": [
        "## How to build a Chatbot for our task?\n",
        "***\n",
        "ChatBots are usually Task specific means if there a chatbot which serves only food delivery app have trained on a dataset which\n",
        "completely different from the dataset on which chatbot which serves online healthcare app. Similary, for this kaggle problem\n",
        "we have provided with movie dataset which may feel that its not specific to any task, but actually it is specific to how people\n",
        "will interect generally as these movie dialogues are nothing but daily life conversation between people however, that chatbot\n",
        "may reply things which sounds too much dramatic and filmy like some dialogue of Tom cruise, shah rukh khan etc.\n",
        "\n",
        "We can approch this problem by applying Neural network models like encoder-decoder architecture with some attention mechanism.\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "425a8749",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "425a8749",
        "outputId": "91ae70cc-bd9b-406e-d8a9-e38dba56f89b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "import numpy as np\n",
        "import codecs\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import ast\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import Progbar\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea97982f",
      "metadata": {
        "id": "ea97982f"
      },
      "source": [
        "## Loading cleaned data that I have preprared while EDA and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c0645f4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "c0645f4e",
        "outputId": "e3ed0151-8671-4b7c-afb8-23279410fda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(139409, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  Well, I thought we'd start with pronunciation,...   \n",
              "1  Not the hacking and gagging and spitting part....   \n",
              "2  You're asking me out.  That's so cute. What's ...   \n",
              "3  No, no, it's my fault -- we didn't have a prop...   \n",
              "4     Gosh, if only we could find Kat a boyfriend...   \n",
              "\n",
              "                                              answer  \n",
              "0  Not the hacking and gagging and spitting part....  \n",
              "1  Okay... then how 'bout we try out some French ...  \n",
              "2                                         Forget it.  \n",
              "3                                           Cameron.  \n",
              "4                          Let me see what I can do.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3bfc0f9-b17a-43fe-828b-322e7ce500c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "      <td>Okay... then how 'bout we try out some French ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
              "      <td>Forget it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
              "      <td>Cameron.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gosh, if only we could find Kat a boyfriend...</td>\n",
              "      <td>Let me see what I can do.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3bfc0f9-b17a-43fe-828b-322e7ce500c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3bfc0f9-b17a-43fe-828b-322e7ce500c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3bfc0f9-b17a-43fe-828b-322e7ce500c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dialogs = pd.read_csv('/content/dialogs_expanded.csv')\n",
        "print(dialogs.shape)\n",
        "dialogs.head()\n",
        "data = dialogs[['question', 'answer']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = joblib.load('data_cleaned')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81af0edc",
      "metadata": {
        "id": "81af0edc"
      },
      "source": [
        "## Dividing into TWO, train/validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5d494f32",
      "metadata": {
        "id": "5d494f32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, validation = train_test_split(data, test_size=0.2, random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ebf65780",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf65780",
        "outputId": "35446e50-a741-4337-bd0e-bd768d47472e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size_ans, vocab_size_ques:69016,69135\n"
          ]
        }
      ],
      "source": [
        "vocab_ans = list(set(\" \".join(train['answer'].values).split()))\n",
        "vocab_ques = list(set(\" \".join(train['question'].values).split()))\n",
        "vocab_size_ans, vocab_size_ques = len(vocab_ans), len(vocab_ques)\n",
        "print(f\"vocab_size_ans, vocab_size_ques:{vocab_size_ans},{ vocab_size_ques}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289182c9",
      "metadata": {
        "id": "289182c9"
      },
      "source": [
        "## Using tfds SubwordTextEncoder, it will create tokens\n",
        "#### example Multiplication -> Multi, pli, cat, i, on \n",
        "#### Advantages: \n",
        "    1. Reduces vocab size => faster learning\n",
        "    2. Reduces chances of missing word in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f322ecb3",
      "metadata": {
        "id": "f322ecb3"
      },
      "outputs": [],
      "source": [
        "tokenizer_a = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train['answer'], target_vocab_size=2**15) \n",
        "\n",
        "tokenizer_q = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train['question'], target_vocab_size=2**15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "609b2718",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "609b2718",
        "outputId": "cb078732-6895-4ddd-a295-ff6a7bfbe4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_q:23598\n",
            "tokenizer_a:30402\n"
          ]
        }
      ],
      "source": [
        "print(f\"tokenizer_q:{tokenizer_q.vocab_size}\")\n",
        "print(f\"tokenizer_a:{tokenizer_a.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02392cc3",
      "metadata": {
        "id": "02392cc3"
      },
      "source": [
        "#### Examples of subword tokenization in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf67e110",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf67e110",
        "outputId": "f20c86e9-94b9-4398-c014-9daa159adbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string is [17990, 3488, 1119, 23930, 8104, 627]\n",
            "The original string: Encoder decoder\n",
            "17990---->En\n",
            "3488---->code\n",
            "1119---->r \n",
            "23930---->dec\n",
            "8104---->od\n",
            "627---->er\n",
            "================================================================================\n",
            "Tokenized string is [16278, 18272, 769, 19843, 23456]\n",
            "The original string: Encoder decoder\n",
            "16278---->Enc\n",
            "18272---->ode\n",
            "769---->r \n",
            "19843---->decode\n",
            "23456---->r\n"
          ]
        }
      ],
      "source": [
        "sample_string = 'Encoder decoder'\n",
        "\n",
        "tokenized_string = tokenizer_a.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_a.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "for token in tokenized_string:\n",
        "    print(str(token) + \"---->\" + tokenizer_a.decode([token]))\n",
        "\n",
        "print(\"=\"*80)\n",
        "tokenized_string = tokenizer_q.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_q.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "for token in tokenized_string:\n",
        "    print(str(token) + \"---->\" + tokenizer_q.decode([token]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9e30ce",
      "metadata": {
        "id": "ff9e30ce"
      },
      "source": [
        "###### 0-27512 for questions\n",
        "\n",
        "###### 0-27357 for answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9469ee",
      "metadata": {
        "id": "3d9469ee"
      },
      "source": [
        "* **Attaching token number '27513' representing \\<start> and '27514' representing \\<end> QUESTIONS**\n",
        "* **Attaching token number '27358' representing \\<start> and '27359' representing \\<end> ANSWERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aeb6d990",
      "metadata": {
        "id": "aeb6d990"
      },
      "outputs": [],
      "source": [
        "def encode(ques, ans):\n",
        "    ques = [tokenizer_q.vocab_size] + tokenizer_q.encode(ques.numpy()) + [tokenizer_q.vocab_size+1]\n",
        "    ans = [tokenizer_a.vocab_size] + tokenizer_a.encode(ans.numpy()) + [tokenizer_a.vocab_size+1]\n",
        "    return ques, ans\n",
        "\n",
        "def tf_encode(ques, ans):\n",
        "    result_ques, result_ans = tf.py_function(encode, [ques, ans], [tf.int64, tf.int64])\n",
        "    result_ques.set_shape([None])\n",
        "    result_ans.set_shape([None])\n",
        "    return result_ques, result_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc78baed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc78baed",
        "outputId": "11ed57ab-9623-4484-bb3e-5aa2a381139b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And the fifty's all gone, huh? Who's the ten for? \n",
            " The Websters.\n",
            "tf.Tensor(\n",
            "[23598    70     6  1443 23381     5    62   627     1   321    35   336\n",
            " 23381     5     6   591   272 23405 23599], shape=(19,), dtype=int64)\n",
            "tf.Tensor([30402    54 25555 30192 30403], shape=(5,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "print(train['question'].values[0],\"\\n\",train['answer'].values[0])\n",
        "question, answer = tf_encode(train['question'].values[0],train['answer'].values[0])\n",
        "print(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1413123",
      "metadata": {
        "id": "d1413123"
      },
      "source": [
        "### Creating train_dataset/test_dataset object from Dataframe + padding\n",
        "\n",
        "###### prefetch: If I'm at epoch-20 then prefetch prepares the Batch for epoch-21, so when epoch-21 start, it will make available the batch in no time, basically enhancing speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "54bb56c4",
      "metadata": {
        "id": "54bb56c4"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(dict(train))\n",
        "train_dataset = train_dataset.map(lambda x:tf_encode(x['question'], x['answer']))\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(20000).padded_batch(64, padded_shapes=([None],[None])) \n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "436c73c8",
      "metadata": {
        "id": "436c73c8"
      },
      "outputs": [],
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices(dict(validation))\n",
        "val_dataset = val_dataset.map(lambda x:tf_encode(x['question'], x['answer']))\n",
        "val_dataset = val_dataset.padded_batch(64, padded_shapes=([None],[None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "85844a50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85844a50",
        "outputId": "4b4be9ed-f9fc-4776-def7-2266fcbeab1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 24), dtype=int64, numpy=\n",
              "array([[23598,   152,   965, ...,     0,     0,     0],\n",
              "       [23598,   244,     1, ...,     0,     0,     0],\n",
              "       [23598,   327,    35, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [23598,     2,    58, ..., 23599,     0,     0],\n",
              "       [23598,  1595,   273, ...,     0,     0,     0],\n",
              "       [23598,     2,    76, ...,     0,     0,     0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "question, answer = next(iter(train_dataset))\n",
        "question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80bf8241",
      "metadata": {
        "id": "80bf8241"
      },
      "source": [
        "### Positional encoding function where 'i' -> embedding dimn index, 'pos' -> word index in a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9500b25e",
      "metadata": {
        "id": "9500b25e"
      },
      "source": [
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ae35c9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2ae35c9d",
        "outputId": "ab32c0cc-eda6-4b0c-df5a-f1449003f2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 50, 512)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Positional encoding\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis,:]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d393354",
      "metadata": {
        "id": "5d393354"
      },
      "source": [
        "## 1. Pad Masking\n",
        "#### Making all the padded tokens, self attention/attention calculation of a word with those paddings will be ignored"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecbd402",
      "metadata": {
        "id": "9ecbd402"
      },
      "source": [
        "\n",
        "* Here output dimn -> (batch_size, 1, 1, seq_len) \n",
        "          \n",
        "*  for each 8 attention heads, for each query word it will be multiplied, thats why creating 1, 1 in the middle\n",
        "\n",
        "##### (batch_size, 8, query_word_len, seq_len) * (batch_size, 1, 1, seq_len) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "628afe59",
      "metadata": {
        "id": "628afe59"
      },
      "outputs": [],
      "source": [
        "# Masking\n",
        "\n",
        "'''Mask all the pad tokens in the batch of sequence. \n",
        "It ensures that the model does not treat padding as the input. \n",
        "The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n",
        "'''\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    seq: padded sentence length (5)\n",
        "    \"\"\"\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # Adding 2, 3 dimn using tf.newaxis, 2-> As this mask will be multiplied with each attention head and 3-> for each word in a sentance\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "# create_padding_mask(np.array([[1,2,3,0,0,0],[1,2,3,0,0,1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9aceca",
      "metadata": {
        "id": "2d9aceca"
      },
      "source": [
        "## 2. Looakahead mask\n",
        "\n",
        "for the first word, its self attention calculation with be ignored with proceeding words i.e. second, third word and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d26dea25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d26dea25",
        "outputId": "cc0d7bab-0964-42e4-9efa-92ee2506fb14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Looakahead mask\n",
        "\n",
        "\"\"\"The look-ahead mask is used to mask the future tokens in a sequence. \n",
        "In other words, the mask indicates which entries should not be used.\n",
        "\"\"\"\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    The look-ahead mask is used to mask the future tokens in a sequence\n",
        "    \"\"\"\n",
        "    #band_part with this setting creates lower triangular matrix that's why subtracting from 1\n",
        "    # [[0., 1., 1.],\n",
        "    #  [0., 0., 1.],\n",
        "    #  [0., 0., 0.]] output with size:3\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "#example\n",
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2896ca7e",
      "metadata": {
        "id": "2896ca7e"
      },
      "source": [
        "## 3. SELF-ATTENTION calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04daf49a",
      "metadata": {
        "id": "04daf49a"
      },
      "source": [
        "![image](images/attenion_formula.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f55e44a8",
      "metadata": {
        "id": "f55e44a8"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth) # NOTE: depth=dk\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\" \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk. underroot d_model i.e. underroot(100)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor. \n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  # -1e9 ~ (-INFINITY) => where ever mask is set, make its logit value close to -INF\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798eca41",
      "metadata": {
        "id": "798eca41"
      },
      "source": [
        "## 4. MultiHeadAttention Calculation\n",
        "#### Its nothing but a RESHAPING !! :)\n",
        "example : \n",
        "1. if we have (64, 10, 512)->(BATCH, #words, embeddding) as input, after passiing it though dense layer of size 512 we will get (64, 10, 512)\n",
        "2. We have three such dense layers representing/for Q, K, V encodings.\n",
        "3. (64, 10, 512) -> reshape -> (64, 8, 10 ,64) -> (BATCH, attention head, #words, encode)\n",
        "    '64' is representing encoding of 512 -> 64 dimension\n",
        "4. (64, 8, 10 ,64)->self-attention-code->(64, 8, 10 ,10) called attention weights, (64, 8, 10 ,64)\n",
        "5. Concatenate such that 8*64 will be new dimension -> (64, 10, 512)\n",
        "    \n",
        "    **Beware embedding dimn must be divisible by no. of heads and always embedding_dimn/heads => encodin_dimn(here 64)**\n",
        "    \n",
        "    **NICE HACK** (-_-)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f41cf3c",
      "metadata": {
        "id": "0f41cf3c"
      },
      "source": [
        "![image](images/multi_head.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ab0b6ec3",
      "metadata": {
        "id": "ab0b6ec3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model  # typically 512\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809f3721",
      "metadata": {
        "id": "809f3721"
      },
      "source": [
        "## 5. ENCODER layer\n",
        "\n",
        "#### -> self Multihead attention -> Residual+Norm -> Feed forward neural network -> Residual+Norm\n",
        "\n",
        "![image](images/encoder_layer.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2c9b69f5",
      "metadata": {
        "id": "2c9b69f5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff): #dff = 512\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model) # with Attention\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model) #with Attention\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d18de1ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d18de1ef",
        "outputId": "2425474b-7970-424b-8ca8-34f8b820cfcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d49552e",
      "metadata": {
        "id": "0d49552e"
      },
      "source": [
        "## 6. DECODER LAYER\n",
        "#### -> self multihead attention -> residual+norm -> multihead attention(between E & D) -> residual+norm -> feed forward NN -> residual+norm\n",
        "\n",
        "![image](images/decoder_layer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f3061bff",
      "metadata": {
        "id": "f3061bff"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "         \n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x) \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2123ca",
      "metadata": {
        "id": "7a2123ca"
      },
      "source": [
        "## 7. ENCODER \n",
        "#### Nothing but repetation of Encoder layer :-) + Input embedding vector + positional encoding\n",
        "\n",
        "![image](images/encoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "98960a2d",
      "metadata": {
        "id": "98960a2d"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]   #x:(batch, seq_len)\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd0dcc2",
      "metadata": {
        "id": "5fd0dcc2"
      },
      "source": [
        "## 8. DECODER\n",
        "#### Nothing but Repetation of decoder layers + posisional encoder + embedding layer\n",
        "\n",
        "![image](images/decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b4232ef9",
      "metadata": {
        "id": "b4232ef9"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50318f4f",
      "metadata": {
        "id": "50318f4f"
      },
      "source": [
        "## 9. TRANSFORMER\n",
        "\n",
        "#### Nothing but encoder+decoder+dense layer\n",
        "##### (64,10,512) -> dense_layer -> (64,10,vocab_size)\n",
        "\n",
        "![image](images/transformer.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6ba6c5e1",
      "metadata": {
        "id": "6ba6c5e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db99e050",
      "metadata": {
        "id": "db99e050"
      },
      "source": [
        "#### So to create a transformer architecture which is now everywhere in NLP models, we require only 9 STEPs :-O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0ad95ab6",
      "metadata": {
        "id": "0ad95ab6"
      },
      "outputs": [],
      "source": [
        "# tokenizer_a = joblib.load(\"tokenizer_a\")\n",
        "# tokenizer_q = joblib.load(\"tokenizer_q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ca0583e3",
      "metadata": {
        "id": "ca0583e3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "fb699bbb",
      "metadata": {
        "id": "fb699bbb"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6dd6833",
      "metadata": {
        "id": "a6dd6833"
      },
      "source": [
        "## Custom learning rate, proposed in the paper\n",
        "#### First learning rate will be high and then after some epochs it will be decreasing ONLY\n",
        "\n",
        "![image](images/lr.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "af0bd674",
      "metadata": {
        "id": "af0bd674"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "95ed1f98",
      "metadata": {
        "id": "95ed1f98"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e3f318",
      "metadata": {
        "id": "a8e3f318"
      },
      "source": [
        "### See, increasing and then decreasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a02400de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "a02400de",
        "outputId": "a8e0d7c1-a6b0-40ad-e684-6a475c7a7019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c461fb",
      "metadata": {
        "id": "05c461fb"
      },
      "source": [
        "##### Custom losss function, same as sparse categorical cross entropy but considers only no padded values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ba02438a",
      "metadata": {
        "id": "ba02438a"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b9a90f",
      "metadata": {
        "id": "01b9a90f"
      },
      "source": [
        "### Creating pad mask(encoder), pad mask(decoder), lookahead mask(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b61f4e3f",
      "metadata": {
        "id": "b61f4e3f"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644505ce",
      "metadata": {
        "id": "644505ce"
      },
      "source": [
        "## Saving model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7a9ba0f8",
      "metadata": {
        "id": "7a9ba0f8"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_path = \"./checkpoints_test/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c3cdf4",
      "metadata": {
        "id": "f4c3cdf4"
      },
      "source": [
        "## Using gradient tape for getting derivatives of loss functions w.r.t. weights then applyiing to optimizer => BACKPROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5e0dc634",
      "metadata": {
        "id": "5e0dc634"
      },
      "outputs": [],
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp) \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                     True, \n",
        "                                     enc_padding_mask, \n",
        "                                     combined_mask, \n",
        "                                     dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf410f58",
      "metadata": {
        "id": "bf410f58"
      },
      "source": [
        "## Creating sample transformer to know the no. of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d0d0ea48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d0ea48",
        "outputId": "d8c9234e-8ccc-4d8b-83ec-2ae4adc9e658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_2 (Encoder)         multiple                  3813888   \n",
            "                                                                 \n",
            " decoder_2 (Decoder)         multiple                  4950016   \n",
            "                                                                 \n",
            " dense_200 (Dense)           multiple                  3922116   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,686,020\n",
            "Trainable params: 12,686,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "\n",
        "sample_transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "sample_transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8d653136",
      "metadata": {
        "id": "8d653136"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47d4b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47d4b94",
        "outputId": "ce59dde8-736a-4839-bcf2-891002ca1c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/20\n",
            " 10432/111527 [=>............................] - ETA: 1:14:08 - loss: 10.0262 - acc: 0.0262"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "train_loss.reset_states()\n",
        "train_accuracy.reset_states()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addc165f",
      "metadata": {
        "id": "addc165f",
        "outputId": "8cd81063-65b4-4d40-b7ef-8e582a70831c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1/5\n",
            "111552/111527 [==============================] - 169s 2ms/step - loss: 4.9995 - acc: 0.0938\n",
            "\n",
            "epoch 2/5\n",
            "111552/111527 [==============================] - 154s 1ms/step - loss: 4.9930 - acc: 0.0941\n",
            "\n",
            "epoch 3/5\n",
            "111552/111527 [==============================] - 215s 2ms/step - loss: 4.9866 - acc: 0.0945\n",
            "\n",
            "epoch 4/5\n",
            "111552/111527 [==============================] - 489s 4ms/step - loss: 4.9803 - acc: 0.0948\n",
            "\n",
            "epoch 5/5\n",
            "111552/111527 [==============================] - 487s 4ms/step - loss: 4.9743 - acc: 0.0951\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-5\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca700ca",
      "metadata": {
        "id": "eca700ca"
      },
      "source": [
        "# With those hyperparameters it seems model is not learning at all, as there is no progress in 25 EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6542bc",
      "metadata": {
        "id": "3c6542bc"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 27 \n",
        "def evaluate(inp_sentence):\n",
        "    start_token = [tokenizer_q.vocab_size]\n",
        "    end_token = [tokenizer_q.vocab_size + 1]\n",
        "\n",
        "    # inp sentence is portuguese, hence adding the start and end token\n",
        "    inp_sentence = start_token + tokenizer_q.encode(inp_sentence) + end_token\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # as the target is english, the first word to the transformer should be the\n",
        "    # english start token.\n",
        "    decoder_input = [tokenizer_a.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                     output,\n",
        "                                                     False,\n",
        "                                                     enc_padding_mask,\n",
        "                                                     combined_mask,\n",
        "                                                     dec_padding_mask)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "#         print(predictions.shape,\"aa\",predictions[: ,-1:, :].shape)\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "#         print(predicted_id, output)\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer_a.vocab_size+1: \n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfbf0c7",
      "metadata": {
        "id": "dcfbf0c7"
      },
      "outputs": [],
      "source": [
        "inp_sentence = \"i am doing great\"\n",
        "a, b = evaluate(inp_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d36412",
      "metadata": {
        "id": "30d36412"
      },
      "source": [
        "# RESULTS are ABSURD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee9871b",
      "metadata": {
        "id": "7ee9871b",
        "outputId": "5eacc375-512b-4c85-ea7a-013b6b41ff07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thel\n",
            "thel\n",
            "thel\n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n",
            "luc \n"
          ]
        }
      ],
      "source": [
        "for i in a[1:]:\n",
        "    print(tokenizer_a.decode([i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45aebc4",
      "metadata": {
        "id": "c45aebc4"
      },
      "source": [
        " ## Lets try with different hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79abb08",
      "metadata": {
        "id": "a79abb08",
        "outputId": "48fa08fb-d63f-4a39-c7df-a977c0e33026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_5 (Encoder)         multiple                  4315008   \n",
            "                                                                 \n",
            " decoder_5 (Decoder)         multiple                  4560384   \n",
            "                                                                 \n",
            " dense_293 (Dense)           multiple                  3529440   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,404,832\n",
            "Trainable params: 12,404,832\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "temp_input = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "sample_transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5709905a",
      "metadata": {
        "id": "5709905a",
        "outputId": "c22ff5f8-8382-4f8d-b873-13c7a8b0ab90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_6 (Encoder)         multiple                  8098048   \n",
            "                                                                 \n",
            " decoder_6 (Decoder)         multiple                  8585728   \n",
            "                                                                 \n",
            " dense_326 (Dense)           multiple                  7031520   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,715,296\n",
            "Trainable params: 23,715,296\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_layers = 2\n",
        "d_model = 256\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "\n",
        "sample_transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 27), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "sample_transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f5656e",
      "metadata": {
        "id": "17f5656e"
      },
      "source": [
        "### Now the trainable parameters has increased from 12M to 24M, Lets test this!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d76c6e",
      "metadata": {
        "id": "27d76c6e",
        "outputId": "4afef3e1-01df-45c7-f0a1-74441ccbe7c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1/20\n",
            "111552/111527 [==============================] - 184s 2ms/step - loss: 5.2070 - acc: 0.0921\n",
            "\n",
            "epoch 2/20\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 4.9807 - acc: 0.0979\n",
            "\n",
            "epoch 3/20\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 4.8595 - acc: 0.1010\n",
            "\n",
            "epoch 4/20\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 4.7736 - acc: 0.1033\n",
            "\n",
            "epoch 5/20\n",
            "111552/111527 [==============================] - 145s 1ms/step - loss: 4.7041 - acc: 0.1053\n",
            "Saving checkpoint for epoch 5 at ./checkpoints2/train\\ckpt-1\n",
            "\n",
            "epoch 6/20\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 4.6442 - acc: 0.1072\n",
            "\n",
            "epoch 7/20\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 4.5910 - acc: 0.1089\n",
            "\n",
            "epoch 8/20\n",
            "111552/111527 [==============================] - 145s 1ms/step - loss: 4.5422 - acc: 0.1107\n",
            "\n",
            "epoch 9/20\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 4.4968 - acc: 0.1123\n",
            "\n",
            "epoch 10/20\n",
            "111552/111527 [==============================] - 145s 1ms/step - loss: 4.4542 - acc: 0.1139\n",
            "Saving checkpoint for epoch 10 at ./checkpoints2/train\\ckpt-2\n",
            "\n",
            "epoch 11/20\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 4.4140 - acc: 0.1155\n",
            "\n",
            "epoch 12/20\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 4.3760 - acc: 0.1171\n",
            "\n",
            "epoch 13/20\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 4.3402 - acc: 0.1186\n",
            "\n",
            "epoch 14/20\n",
            "111552/111527 [==============================] - 145s 1ms/step - loss: 4.3065 - acc: 0.1201\n",
            "\n",
            "epoch 15/20\n",
            "111552/111527 [==============================] - 145s 1ms/step - loss: 4.2753 - acc: 0.1216\n",
            "Saving checkpoint for epoch 15 at ./checkpoints2/train\\ckpt-3\n",
            "\n",
            "epoch 16/20\n",
            "111552/111527 [==============================] - 146s 1ms/step - loss: 4.2467 - acc: 0.1230\n",
            "\n",
            "epoch 17/20\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 4.2203 - acc: 0.1245\n",
            "\n",
            "epoch 18/20\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 4.1958 - acc: 0.1258\n",
            "\n",
            "epoch 19/20\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 4.1724 - acc: 0.1271\n",
            "\n",
            "epoch 20/20\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 4.1498 - acc: 0.1285\n",
            "Saving checkpoint for epoch 20 at ./checkpoints2/train\\ckpt-4\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "train_loss.reset_states()\n",
        "train_accuracy.reset_states()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8990f5ec",
      "metadata": {
        "id": "8990f5ec",
        "outputId": "bc2f41a7-74f1-4617-ed0e-ea500d74ad9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1/30\n",
            "111552/111527 [==============================] - 146s 1ms/step - loss: 4.1279 - acc: 0.1297\n",
            "\n",
            "epoch 2/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 4.1067 - acc: 0.1310\n",
            "\n",
            "epoch 3/30\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 4.0860 - acc: 0.1322\n",
            "\n",
            "epoch 4/30\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 4.0660 - acc: 0.1334\n",
            "\n",
            "epoch 5/30\n",
            "111552/111527 [==============================] - 146s 1ms/step - loss: 4.0465 - acc: 0.1346\n",
            "Saving checkpoint for epoch 5 at ./checkpoints2/train\\ckpt-5\n",
            "\n",
            "epoch 6/30\n",
            "111552/111527 [==============================] - 147s 1ms/step - loss: 4.0276 - acc: 0.1358\n",
            "\n",
            "epoch 7/30\n",
            "111552/111527 [==============================] - 149s 1ms/step - loss: 4.0091 - acc: 0.1369\n",
            "\n",
            "epoch 8/30\n",
            "111552/111527 [==============================] - 149s 1ms/step - loss: 3.9912 - acc: 0.1381\n",
            "\n",
            "epoch 9/30\n",
            "111552/111527 [==============================] - 147s 1ms/step - loss: 3.9736 - acc: 0.1392\n",
            "\n",
            "epoch 10/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 3.9566 - acc: 0.1402\n",
            "Saving checkpoint for epoch 10 at ./checkpoints2/train\\ckpt-6\n",
            "\n",
            "epoch 11/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 3.9399 - acc: 0.1413\n",
            "\n",
            "epoch 12/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 3.9237 - acc: 0.1423\n",
            "\n",
            "epoch 13/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 3.9078 - acc: 0.1434\n",
            "\n",
            "epoch 14/30\n",
            "111552/111527 [==============================] - 149s 1ms/step - loss: 3.8923 - acc: 0.1444\n",
            "\n",
            "epoch 15/30\n",
            "111552/111527 [==============================] - 149s 1ms/step - loss: 3.8773 - acc: 0.1453\n",
            "Saving checkpoint for epoch 15 at ./checkpoints2/train\\ckpt-7\n",
            "\n",
            "epoch 16/30\n",
            "111552/111527 [==============================] - 149s 1ms/step - loss: 3.8624 - acc: 0.1463\n",
            "\n",
            "epoch 17/30\n",
            "111552/111527 [==============================] - 148s 1ms/step - loss: 3.8478 - acc: 0.1473\n",
            "\n",
            "epoch 18/30\n",
            "111552/111527 [==============================] - 151s 1ms/step - loss: 3.8336 - acc: 0.1482\n",
            "\n",
            "epoch 19/30\n",
            "111552/111527 [==============================] - 153s 1ms/step - loss: 3.8196 - acc: 0.1491\n",
            "\n",
            "epoch 20/30\n",
            "111552/111527 [==============================] - 151s 1ms/step - loss: 3.8059 - acc: 0.1500\n",
            "Saving checkpoint for epoch 20 at ./checkpoints2/train\\ckpt-8\n",
            "\n",
            "epoch 21/30\n",
            "111552/111527 [==============================] - 151s 1ms/step - loss: 3.7925 - acc: 0.1509\n",
            "\n",
            "epoch 22/30\n",
            "111552/111527 [==============================] - 153s 1ms/step - loss: 3.7793 - acc: 0.1518\n",
            "\n",
            "epoch 23/30\n",
            "111552/111527 [==============================] - 150s 1ms/step - loss: 3.7663 - acc: 0.1526\n",
            "\n",
            "epoch 24/30\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.7536 - acc: 0.1534\n",
            "\n",
            "epoch 25/30\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.7411 - acc: 0.1543\n",
            "Saving checkpoint for epoch 25 at ./checkpoints2/train\\ckpt-9\n",
            "\n",
            "epoch 26/30\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.7288 - acc: 0.1551\n",
            "\n",
            "epoch 27/30\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.7167 - acc: 0.1559\n",
            "\n",
            "epoch 28/30\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.7048 - acc: 0.1567\n",
            "\n",
            "epoch 29/30\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.6931 - acc: 0.1574\n",
            "\n",
            "epoch 30/30\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.6816 - acc: 0.1582\n",
            "Saving checkpoint for epoch 30 at ./checkpoints2/train\\ckpt-10\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e364afa5",
      "metadata": {
        "id": "e364afa5"
      },
      "source": [
        "## Learning is slow, but not stagnant (-__-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47943fa8",
      "metadata": {
        "id": "47943fa8",
        "outputId": "8caeb36c-7748-4045-8376-532d8b815641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 51/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.6703 - acc: 0.1589\n",
            "\n",
            "epoch 52/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.6591 - acc: 0.1597\n",
            "\n",
            "epoch 53/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.6481 - acc: 0.1604\n",
            "\n",
            "epoch 54/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.6373 - acc: 0.1611\n",
            "\n",
            "epoch 55/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.6266 - acc: 0.1618\n",
            "Saving checkpoint for epoch 55 at ./checkpoints2/train\\ckpt-11\n",
            "\n",
            "epoch 56/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.6161 - acc: 0.1625\n",
            "\n",
            "epoch 57/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.6057 - acc: 0.1632\n",
            "\n",
            "epoch 58/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.5955 - acc: 0.1639\n",
            "\n",
            "epoch 59/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.5855 - acc: 0.1646\n",
            "\n",
            "epoch 60/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.5755 - acc: 0.1652\n",
            "Saving checkpoint for epoch 60 at ./checkpoints2/train\\ckpt-12\n",
            "\n",
            "epoch 61/100\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 3.5657 - acc: 0.1659\n",
            "\n",
            "epoch 62/100\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 3.5561 - acc: 0.1665\n",
            "\n",
            "epoch 63/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.5465 - acc: 0.1671\n",
            "\n",
            "epoch 64/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.5371 - acc: 0.1678\n",
            "\n",
            "epoch 65/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.5278 - acc: 0.1684\n",
            "Saving checkpoint for epoch 65 at ./checkpoints2/train\\ckpt-13\n",
            "\n",
            "epoch 66/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.5186 - acc: 0.1690\n",
            "\n",
            "epoch 67/100\n",
            "111552/111527 [==============================] - 265s 2ms/step - loss: 3.5094 - acc: 0.1696\n",
            "\n",
            "epoch 68/100\n",
            "111552/111527 [==============================] - 364s 3ms/step - loss: 3.5004 - acc: 0.1702\n",
            "\n",
            "epoch 69/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.4914 - acc: 0.1708\n",
            "\n",
            "epoch 70/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.4825 - acc: 0.1714\n",
            "Saving checkpoint for epoch 70 at ./checkpoints2/train\\ckpt-14\n",
            "\n",
            "epoch 71/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.4737 - acc: 0.1719\n",
            "\n",
            "epoch 72/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.4650 - acc: 0.1725\n",
            "\n",
            "epoch 73/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.4564 - acc: 0.1730\n",
            "\n",
            "epoch 74/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.4479 - acc: 0.1736\n",
            "\n",
            "epoch 75/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.4394 - acc: 0.1741\n",
            "Saving checkpoint for epoch 75 at ./checkpoints2/train\\ckpt-15\n",
            "\n",
            "epoch 76/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.4311 - acc: 0.1747\n",
            "\n",
            "epoch 77/100\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.4229 - acc: 0.1752\n",
            "\n",
            "epoch 78/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.4148 - acc: 0.1757\n",
            "\n",
            "epoch 79/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.4068 - acc: 0.1762\n",
            "\n",
            "epoch 80/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3989 - acc: 0.1768\n",
            "Saving checkpoint for epoch 80 at ./checkpoints2/train\\ckpt-16\n",
            "\n",
            "epoch 81/100\n",
            "111552/111527 [==============================] - 140s 1ms/step - loss: 3.3912 - acc: 0.1773\n",
            "\n",
            "epoch 82/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3837 - acc: 0.1778\n",
            "\n",
            "epoch 83/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3762 - acc: 0.1783\n",
            "\n",
            "epoch 84/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3689 - acc: 0.1787\n",
            "\n",
            "epoch 85/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3617 - acc: 0.1792\n",
            "Saving checkpoint for epoch 85 at ./checkpoints2/train\\ckpt-17\n",
            "\n",
            "epoch 86/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.3546 - acc: 0.1797\n",
            "\n",
            "epoch 87/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.3476 - acc: 0.1802\n",
            "\n",
            "epoch 88/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3407 - acc: 0.1807\n",
            "\n",
            "epoch 89/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3339 - acc: 0.1811\n",
            "\n",
            "epoch 90/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.3273 - acc: 0.1816\n",
            "Saving checkpoint for epoch 90 at ./checkpoints2/train\\ckpt-18\n",
            "\n",
            "epoch 91/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3206 - acc: 0.1820\n",
            "\n",
            "epoch 92/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3141 - acc: 0.1825\n",
            "\n",
            "epoch 93/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3077 - acc: 0.1829\n",
            "\n",
            "epoch 94/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.3012 - acc: 0.1834\n",
            "\n",
            "epoch 95/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2949 - acc: 0.1838\n",
            "Saving checkpoint for epoch 95 at ./checkpoints2/train\\ckpt-19\n",
            "\n",
            "epoch 96/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2886 - acc: 0.1843\n",
            "\n",
            "epoch 97/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2824 - acc: 0.1847\n",
            "\n",
            "epoch 98/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2763 - acc: 0.1851\n",
            "\n",
            "epoch 99/100\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2702 - acc: 0.1855\n",
            "\n",
            "epoch 100/100\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2642 - acc: 0.1860\n",
            "Saving checkpoint for epoch 100 at ./checkpoints2/train\\ckpt-20\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(50, EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c533244c",
      "metadata": {
        "id": "c533244c",
        "outputId": "4b1d8954-b21e-4f7c-ad74-ea43ce3e1d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 101/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2584 - acc: 0.1864\n",
            "\n",
            "epoch 102/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2525 - acc: 0.1868\n",
            "\n",
            "epoch 103/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2468 - acc: 0.1872\n",
            "\n",
            "epoch 104/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2411 - acc: 0.1876\n",
            "\n",
            "epoch 105/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2355 - acc: 0.1880\n",
            "Saving checkpoint for epoch 105 at ./checkpoints2/train\\ckpt-21\n",
            "\n",
            "epoch 106/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2300 - acc: 0.1884\n",
            "\n",
            "epoch 107/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.2245 - acc: 0.1888\n",
            "\n",
            "epoch 108/150\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.2192 - acc: 0.1892\n",
            "\n",
            "epoch 109/150\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.2139 - acc: 0.1895\n",
            "\n",
            "epoch 110/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2086 - acc: 0.1899\n",
            "Saving checkpoint for epoch 110 at ./checkpoints2/train\\ckpt-22\n",
            "\n",
            "epoch 111/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.2034 - acc: 0.1903\n",
            "\n",
            "epoch 112/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1983 - acc: 0.1907\n",
            "\n",
            "epoch 113/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1932 - acc: 0.1910\n",
            "\n",
            "epoch 114/150\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.1881 - acc: 0.1914\n",
            "\n",
            "epoch 115/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1831 - acc: 0.1918\n",
            "Saving checkpoint for epoch 115 at ./checkpoints2/train\\ckpt-23\n",
            "\n",
            "epoch 116/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1781 - acc: 0.1921\n",
            "\n",
            "epoch 117/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1732 - acc: 0.1925\n",
            "\n",
            "epoch 118/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1683 - acc: 0.1928\n",
            "\n",
            "epoch 119/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1634 - acc: 0.1932\n",
            "\n",
            "epoch 120/150\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 3.1586 - acc: 0.1936\n",
            "Saving checkpoint for epoch 120 at ./checkpoints2/train\\ckpt-24\n",
            "\n",
            "epoch 121/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1538 - acc: 0.1939\n",
            "\n",
            "epoch 122/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1490 - acc: 0.1942\n",
            "\n",
            "epoch 123/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1442 - acc: 0.1946\n",
            "\n",
            "epoch 124/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1395 - acc: 0.1949\n",
            "\n",
            "epoch 125/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1348 - acc: 0.1953\n",
            "Saving checkpoint for epoch 125 at ./checkpoints2/train\\ckpt-25\n",
            "\n",
            "epoch 126/150\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 3.1301 - acc: 0.1956\n",
            "\n",
            "epoch 127/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1255 - acc: 0.1960\n",
            "\n",
            "epoch 128/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1209 - acc: 0.1963\n",
            "\n",
            "epoch 129/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1163 - acc: 0.1966\n",
            "\n",
            "epoch 130/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1118 - acc: 0.1969\n",
            "Saving checkpoint for epoch 130 at ./checkpoints2/train\\ckpt-26\n",
            "\n",
            "epoch 131/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1072 - acc: 0.1973\n",
            "\n",
            "epoch 132/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.1027 - acc: 0.1976\n",
            "\n",
            "epoch 133/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0983 - acc: 0.1979\n",
            "\n",
            "epoch 134/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0938 - acc: 0.1982\n",
            "\n",
            "epoch 135/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0894 - acc: 0.1986\n",
            "Saving checkpoint for epoch 135 at ./checkpoints2/train\\ckpt-27\n",
            "\n",
            "epoch 136/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0850 - acc: 0.1989\n",
            "\n",
            "epoch 137/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0806 - acc: 0.1992\n",
            "\n",
            "epoch 138/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0763 - acc: 0.1995\n",
            "\n",
            "epoch 139/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0720 - acc: 0.1998\n",
            "\n",
            "epoch 140/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0677 - acc: 0.2001\n",
            "Saving checkpoint for epoch 140 at ./checkpoints2/train\\ckpt-28\n",
            "\n",
            "epoch 141/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0635 - acc: 0.2004\n",
            "\n",
            "epoch 142/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0593 - acc: 0.2007\n",
            "\n",
            "epoch 143/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0550 - acc: 0.2010\n",
            "\n",
            "epoch 144/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0509 - acc: 0.2013\n",
            "\n",
            "epoch 145/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0467 - acc: 0.2016\n",
            "Saving checkpoint for epoch 145 at ./checkpoints2/train\\ckpt-29\n",
            "\n",
            "epoch 146/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0426 - acc: 0.2019\n",
            "\n",
            "epoch 147/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0385 - acc: 0.2022\n",
            "\n",
            "epoch 148/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0344 - acc: 0.2025\n",
            "\n",
            "epoch 149/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0304 - acc: 0.2028\n",
            "\n",
            "epoch 150/150\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 3.0263 - acc: 0.2031\n",
            "Saving checkpoint for epoch 150 at ./checkpoints2/train\\ckpt-30\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 150\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(100, EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebb9b2d",
      "metadata": {
        "id": "7ebb9b2d",
        "outputId": "c5c69068-18d2-42b5-f22a-9df11d86703a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 151/200\n",
            "111552/111527 [==============================] - 140s 1ms/step - loss: 3.0223 - acc: 0.2033\n",
            "\n",
            "epoch 152/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.0183 - acc: 0.2036\n",
            "\n",
            "epoch 153/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.0144 - acc: 0.2039\n",
            "\n",
            "epoch 154/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.0105 - acc: 0.2042\n",
            "\n",
            "epoch 155/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.0066 - acc: 0.2045\n",
            "Saving checkpoint for epoch 155 at ./checkpoints2/train\\ckpt-31\n",
            "\n",
            "epoch 156/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 3.0027 - acc: 0.2047\n",
            "\n",
            "epoch 157/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9989 - acc: 0.2050\n",
            "\n",
            "epoch 158/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9950 - acc: 0.2053\n",
            "\n",
            "epoch 159/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9912 - acc: 0.2056\n",
            "\n",
            "epoch 160/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9875 - acc: 0.2058\n",
            "Saving checkpoint for epoch 160 at ./checkpoints2/train\\ckpt-32\n",
            "\n",
            "epoch 161/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9837 - acc: 0.2061\n",
            "\n",
            "epoch 162/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9800 - acc: 0.2063\n",
            "\n",
            "epoch 163/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9763 - acc: 0.2066\n",
            "\n",
            "epoch 164/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9726 - acc: 0.2069\n",
            "\n",
            "epoch 165/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9689 - acc: 0.2071\n",
            "Saving checkpoint for epoch 165 at ./checkpoints2/train\\ckpt-33\n",
            "\n",
            "epoch 166/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9653 - acc: 0.2074\n",
            "\n",
            "epoch 167/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9617 - acc: 0.2077\n",
            "\n",
            "epoch 168/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9581 - acc: 0.2079\n",
            "\n",
            "epoch 169/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9546 - acc: 0.2082\n",
            "\n",
            "epoch 170/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9510 - acc: 0.2084\n",
            "Saving checkpoint for epoch 170 at ./checkpoints2/train\\ckpt-34\n",
            "\n",
            "epoch 171/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9475 - acc: 0.2087\n",
            "\n",
            "epoch 172/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9440 - acc: 0.2089\n",
            "\n",
            "epoch 173/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9406 - acc: 0.2092\n",
            "\n",
            "epoch 174/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.9371 - acc: 0.2094\n",
            "\n",
            "epoch 175/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9337 - acc: 0.2097\n",
            "Saving checkpoint for epoch 175 at ./checkpoints2/train\\ckpt-35\n",
            "\n",
            "epoch 176/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9303 - acc: 0.2099\n",
            "\n",
            "epoch 177/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9269 - acc: 0.2101\n",
            "\n",
            "epoch 178/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9235 - acc: 0.2104\n",
            "\n",
            "epoch 179/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9202 - acc: 0.2106\n",
            "\n",
            "epoch 180/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9168 - acc: 0.2109\n",
            "Saving checkpoint for epoch 180 at ./checkpoints2/train\\ckpt-36\n",
            "\n",
            "epoch 181/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9135 - acc: 0.2111\n",
            "\n",
            "epoch 182/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9102 - acc: 0.2113\n",
            "\n",
            "epoch 183/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9069 - acc: 0.2116\n",
            "\n",
            "epoch 184/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9037 - acc: 0.2118\n",
            "\n",
            "epoch 185/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.9004 - acc: 0.2120\n",
            "Saving checkpoint for epoch 185 at ./checkpoints2/train\\ckpt-37\n",
            "\n",
            "epoch 186/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8972 - acc: 0.2123\n",
            "\n",
            "epoch 187/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8941 - acc: 0.2125\n",
            "\n",
            "epoch 188/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8909 - acc: 0.2127\n",
            "\n",
            "epoch 189/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8877 - acc: 0.2129\n",
            "\n",
            "epoch 190/200\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8846 - acc: 0.2132\n",
            "Saving checkpoint for epoch 190 at ./checkpoints2/train\\ckpt-38\n",
            "\n",
            "epoch 191/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8815 - acc: 0.2134\n",
            "\n",
            "epoch 192/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8784 - acc: 0.2136\n",
            "\n",
            "epoch 193/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8753 - acc: 0.2138\n",
            "\n",
            "epoch 194/200\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8722 - acc: 0.2141\n",
            "\n",
            "epoch 195/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8692 - acc: 0.2143\n",
            "Saving checkpoint for epoch 195 at ./checkpoints2/train\\ckpt-39\n",
            "\n",
            "epoch 196/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8662 - acc: 0.2145\n",
            "\n",
            "epoch 197/200\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8632 - acc: 0.2147\n",
            "\n",
            "epoch 198/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8602 - acc: 0.2149\n",
            "\n",
            "epoch 199/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8572 - acc: 0.2151\n",
            "\n",
            "epoch 200/200\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8543 - acc: 0.2153\n",
            "Saving checkpoint for epoch 200 at ./checkpoints2/train\\ckpt-40\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 200\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(150, EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa114565",
      "metadata": {
        "id": "fa114565",
        "outputId": "811c5a5a-4f4e-4b2c-c010-911bebebec0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 201/300\n",
            "111552/111527 [==============================] - 140s 1ms/step - loss: 2.8513 - acc: 0.2156\n",
            "\n",
            "epoch 202/300\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.8484 - acc: 0.2158\n",
            "\n",
            "epoch 203/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8455 - acc: 0.2160\n",
            "\n",
            "epoch 204/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8426 - acc: 0.2162\n",
            "\n",
            "epoch 205/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8398 - acc: 0.2164\n",
            "Saving checkpoint for epoch 205 at ./checkpoints2/train\\ckpt-41\n",
            "\n",
            "epoch 206/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8369 - acc: 0.2166\n",
            "\n",
            "epoch 207/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8341 - acc: 0.2168\n",
            "\n",
            "epoch 208/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8312 - acc: 0.2170\n",
            "\n",
            "epoch 209/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8284 - acc: 0.2172\n",
            "\n",
            "epoch 210/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8256 - acc: 0.2174\n",
            "Saving checkpoint for epoch 210 at ./checkpoints2/train\\ckpt-42\n",
            "\n",
            "epoch 211/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8228 - acc: 0.2176\n",
            "\n",
            "epoch 212/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8201 - acc: 0.2178\n",
            "\n",
            "epoch 213/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8173 - acc: 0.2180\n",
            "\n",
            "epoch 214/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.8146 - acc: 0.2182\n",
            "\n",
            "epoch 215/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8118 - acc: 0.2184\n",
            "Saving checkpoint for epoch 215 at ./checkpoints2/train\\ckpt-43\n",
            "\n",
            "epoch 216/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8091 - acc: 0.2186\n",
            "\n",
            "epoch 217/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8064 - acc: 0.2188\n",
            "\n",
            "epoch 218/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8037 - acc: 0.2190\n",
            "\n",
            "epoch 219/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.8011 - acc: 0.2192\n",
            "\n",
            "epoch 220/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7984 - acc: 0.2194\n",
            "Saving checkpoint for epoch 220 at ./checkpoints2/train\\ckpt-44\n",
            "\n",
            "epoch 221/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7958 - acc: 0.2195\n",
            "\n",
            "epoch 222/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7931 - acc: 0.2197\n",
            "\n",
            "epoch 223/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7905 - acc: 0.2199\n",
            "\n",
            "epoch 224/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7879 - acc: 0.2201\n",
            "\n",
            "epoch 225/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7853 - acc: 0.2203\n",
            "Saving checkpoint for epoch 225 at ./checkpoints2/train\\ckpt-45\n",
            "\n",
            "epoch 226/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7827 - acc: 0.2205\n",
            "\n",
            "epoch 227/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7802 - acc: 0.2207\n",
            "\n",
            "epoch 228/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7776 - acc: 0.2208\n",
            "\n",
            "epoch 229/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7751 - acc: 0.2210\n",
            "\n",
            "epoch 230/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7726 - acc: 0.2212\n",
            "Saving checkpoint for epoch 230 at ./checkpoints2/train\\ckpt-46\n",
            "\n",
            "epoch 231/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7700 - acc: 0.2214\n",
            "\n",
            "epoch 232/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7675 - acc: 0.2216\n",
            "\n",
            "epoch 233/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7651 - acc: 0.2217\n",
            "\n",
            "epoch 234/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7626 - acc: 0.2219\n",
            "\n",
            "epoch 235/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7601 - acc: 0.2221\n",
            "Saving checkpoint for epoch 235 at ./checkpoints2/train\\ckpt-47\n",
            "\n",
            "epoch 236/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7577 - acc: 0.2223\n",
            "\n",
            "epoch 237/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7552 - acc: 0.2225\n",
            "\n",
            "epoch 238/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7528 - acc: 0.2226\n",
            "\n",
            "epoch 239/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7504 - acc: 0.2228\n",
            "\n",
            "epoch 240/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7481 - acc: 0.2230\n",
            "Saving checkpoint for epoch 240 at ./checkpoints2/train\\ckpt-48\n",
            "\n",
            "epoch 241/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7457 - acc: 0.2231\n",
            "\n",
            "epoch 242/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7433 - acc: 0.2233\n",
            "\n",
            "epoch 243/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7410 - acc: 0.2235\n",
            "\n",
            "epoch 244/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7386 - acc: 0.2237\n",
            "\n",
            "epoch 245/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7363 - acc: 0.2238\n",
            "Saving checkpoint for epoch 245 at ./checkpoints2/train\\ckpt-49\n",
            "\n",
            "epoch 246/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7339 - acc: 0.2240\n",
            "\n",
            "epoch 247/300\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.7316 - acc: 0.2242\n",
            "\n",
            "epoch 248/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7292 - acc: 0.2243\n",
            "\n",
            "epoch 249/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7269 - acc: 0.2245\n",
            "\n",
            "epoch 250/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7245 - acc: 0.2247\n",
            "Saving checkpoint for epoch 250 at ./checkpoints2/train\\ckpt-50\n",
            "\n",
            "epoch 251/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7222 - acc: 0.2248\n",
            "\n",
            "epoch 252/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7199 - acc: 0.2250\n",
            "\n",
            "epoch 253/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7175 - acc: 0.2252\n",
            "\n",
            "epoch 254/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7152 - acc: 0.2253\n",
            "\n",
            "epoch 255/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7129 - acc: 0.2255\n",
            "Saving checkpoint for epoch 255 at ./checkpoints2/train\\ckpt-51\n",
            "\n",
            "epoch 256/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7107 - acc: 0.2256\n",
            "\n",
            "epoch 257/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7084 - acc: 0.2258\n",
            "\n",
            "epoch 258/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7062 - acc: 0.2260\n",
            "\n",
            "epoch 259/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.7039 - acc: 0.2261\n",
            "\n",
            "epoch 260/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.7017 - acc: 0.2263\n",
            "Saving checkpoint for epoch 260 at ./checkpoints2/train\\ckpt-52\n",
            "\n",
            "epoch 261/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6994 - acc: 0.2264\n",
            "\n",
            "epoch 262/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6972 - acc: 0.2266\n",
            "\n",
            "epoch 263/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6950 - acc: 0.2267\n",
            "\n",
            "epoch 264/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6928 - acc: 0.2269\n",
            "\n",
            "epoch 265/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6906 - acc: 0.2271\n",
            "Saving checkpoint for epoch 265 at ./checkpoints2/train\\ckpt-53\n",
            "\n",
            "epoch 266/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6884 - acc: 0.2272\n",
            "\n",
            "epoch 267/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6862 - acc: 0.2274\n",
            "\n",
            "epoch 268/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6841 - acc: 0.2275\n",
            "\n",
            "epoch 269/300\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 2.6819 - acc: 0.2277\n",
            "\n",
            "epoch 270/300\n",
            "111552/111527 [==============================] - 144s 1ms/step - loss: 2.6797 - acc: 0.2278\n",
            "Saving checkpoint for epoch 270 at ./checkpoints2/train\\ckpt-54\n",
            "\n",
            "epoch 271/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6776 - acc: 0.2280\n",
            "\n",
            "epoch 272/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6754 - acc: 0.2281\n",
            "\n",
            "epoch 273/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6733 - acc: 0.2283\n",
            "\n",
            "epoch 274/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6711 - acc: 0.2284\n",
            "\n",
            "epoch 275/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6690 - acc: 0.2286\n",
            "Saving checkpoint for epoch 275 at ./checkpoints2/train\\ckpt-55\n",
            "\n",
            "epoch 276/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6668 - acc: 0.2287\n",
            "\n",
            "epoch 277/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6647 - acc: 0.2289\n",
            "\n",
            "epoch 278/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6626 - acc: 0.2290\n",
            "\n",
            "epoch 279/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6605 - acc: 0.2292\n",
            "\n",
            "epoch 280/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6584 - acc: 0.2293\n",
            "Saving checkpoint for epoch 280 at ./checkpoints2/train\\ckpt-56\n",
            "\n",
            "epoch 281/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6563 - acc: 0.2295\n",
            "\n",
            "epoch 282/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6542 - acc: 0.2296\n",
            "\n",
            "epoch 283/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6521 - acc: 0.2298\n",
            "\n",
            "epoch 284/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6500 - acc: 0.2299\n",
            "\n",
            "epoch 285/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6480 - acc: 0.2301\n",
            "Saving checkpoint for epoch 285 at ./checkpoints2/train\\ckpt-57\n",
            "\n",
            "epoch 286/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6459 - acc: 0.2302\n",
            "\n",
            "epoch 287/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6439 - acc: 0.2304\n",
            "\n",
            "epoch 288/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6418 - acc: 0.2305\n",
            "\n",
            "epoch 289/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6398 - acc: 0.2306\n",
            "\n",
            "epoch 290/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6378 - acc: 0.2308\n",
            "Saving checkpoint for epoch 290 at ./checkpoints2/train\\ckpt-58\n",
            "\n",
            "epoch 291/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6357 - acc: 0.2309\n",
            "\n",
            "epoch 292/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6337 - acc: 0.2311\n",
            "\n",
            "epoch 293/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6317 - acc: 0.2312\n",
            "\n",
            "epoch 294/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6298 - acc: 0.2313\n",
            "\n",
            "epoch 295/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6278 - acc: 0.2315\n",
            "Saving checkpoint for epoch 295 at ./checkpoints2/train\\ckpt-59\n",
            "\n",
            "epoch 296/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6258 - acc: 0.2316\n",
            "\n",
            "epoch 297/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6239 - acc: 0.2318\n",
            "\n",
            "epoch 298/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6220 - acc: 0.2319\n",
            "\n",
            "epoch 299/300\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.6201 - acc: 0.2320\n",
            "\n",
            "epoch 300/300\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6182 - acc: 0.2322\n",
            "Saving checkpoint for epoch 300 at ./checkpoints2/train\\ckpt-60\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 300\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(200, EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88154466",
      "metadata": {
        "id": "88154466",
        "outputId": "6a27b619-b331-49d3-fef8-b68292e6fc6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 301/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6134 - acc: 0.2325\n",
            "\n",
            "epoch 302/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6115 - acc: 0.2326\n",
            "\n",
            "epoch 303/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6096 - acc: 0.2328\n",
            "\n",
            "epoch 304/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6078 - acc: 0.2329\n",
            "\n",
            "epoch 305/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.6059 - acc: 0.2330\n",
            "Saving checkpoint for epoch 305 at ./checkpoints2/train\\ckpt-61\n",
            "\n",
            "epoch 306/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.6041 - acc: 0.2331\n",
            "\n",
            "epoch 307/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.6023 - acc: 0.2333\n",
            "\n",
            "epoch 308/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.6005 - acc: 0.2334\n",
            "\n",
            "epoch 309/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5986 - acc: 0.2335\n",
            "\n",
            "epoch 310/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5968 - acc: 0.2337\n",
            "Saving checkpoint for epoch 310 at ./checkpoints2/train\\ckpt-62\n",
            "\n",
            "epoch 311/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5950 - acc: 0.2338\n",
            "\n",
            "epoch 312/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5932 - acc: 0.2339\n",
            "\n",
            "epoch 313/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5914 - acc: 0.2340\n",
            "\n",
            "epoch 314/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5896 - acc: 0.2342\n",
            "\n",
            "epoch 315/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5878 - acc: 0.2343\n",
            "Saving checkpoint for epoch 315 at ./checkpoints2/train\\ckpt-63\n",
            "\n",
            "epoch 316/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5861 - acc: 0.2344\n",
            "\n",
            "epoch 317/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5843 - acc: 0.2345\n",
            "\n",
            "epoch 318/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5825 - acc: 0.2346\n",
            "\n",
            "epoch 319/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5808 - acc: 0.2348\n",
            "\n",
            "epoch 320/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5790 - acc: 0.2349\n",
            "Saving checkpoint for epoch 320 at ./checkpoints2/train\\ckpt-64\n",
            "\n",
            "epoch 321/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5773 - acc: 0.2350\n",
            "\n",
            "epoch 322/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5755 - acc: 0.2351\n",
            "\n",
            "epoch 323/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5738 - acc: 0.2352\n",
            "\n",
            "epoch 324/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5721 - acc: 0.2354\n",
            "\n",
            "epoch 325/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5704 - acc: 0.2355\n",
            "Saving checkpoint for epoch 325 at ./checkpoints2/train\\ckpt-65\n",
            "\n",
            "epoch 326/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5686 - acc: 0.2356\n",
            "\n",
            "epoch 327/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5670 - acc: 0.2357\n",
            "\n",
            "epoch 328/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5653 - acc: 0.2358\n",
            "\n",
            "epoch 329/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5636 - acc: 0.2360\n",
            "\n",
            "epoch 330/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5619 - acc: 0.2361\n",
            "Saving checkpoint for epoch 330 at ./checkpoints2/train\\ckpt-66\n",
            "\n",
            "epoch 331/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5602 - acc: 0.2362\n",
            "\n",
            "epoch 332/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5586 - acc: 0.2363\n",
            "\n",
            "epoch 333/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5569 - acc: 0.2364\n",
            "\n",
            "epoch 334/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5553 - acc: 0.2365\n",
            "\n",
            "epoch 335/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5536 - acc: 0.2367\n",
            "Saving checkpoint for epoch 335 at ./checkpoints2/train\\ckpt-67\n",
            "\n",
            "epoch 336/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5520 - acc: 0.2368\n",
            "\n",
            "epoch 337/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5504 - acc: 0.2369\n",
            "\n",
            "epoch 338/350\n",
            "111552/111527 [==============================] - 141s 1ms/step - loss: 2.5488 - acc: 0.2370\n",
            "\n",
            "epoch 339/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5472 - acc: 0.2371\n",
            "\n",
            "epoch 340/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5456 - acc: 0.2372\n",
            "Saving checkpoint for epoch 340 at ./checkpoints2/train\\ckpt-68\n",
            "\n",
            "epoch 341/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5440 - acc: 0.2374\n",
            "\n",
            "epoch 342/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5424 - acc: 0.2375\n",
            "\n",
            "epoch 343/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5408 - acc: 0.2376\n",
            "\n",
            "epoch 344/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5393 - acc: 0.2377\n",
            "\n",
            "epoch 345/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5377 - acc: 0.2378\n",
            "Saving checkpoint for epoch 345 at ./checkpoints2/train\\ckpt-69\n",
            "\n",
            "epoch 346/350\n",
            "111552/111527 [==============================] - 142s 1ms/step - loss: 2.5362 - acc: 0.2379\n",
            "\n",
            "epoch 347/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5347 - acc: 0.2380\n",
            "\n",
            "epoch 348/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5332 - acc: 0.2381\n",
            "\n",
            "epoch 349/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5316 - acc: 0.2383\n",
            "\n",
            "epoch 350/350\n",
            "111552/111527 [==============================] - 143s 1ms/step - loss: 2.5301 - acc: 0.2384\n",
            "Saving checkpoint for epoch 350 at ./checkpoints2/train\\ckpt-70\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 350\n",
        "batch_size = 64\n",
        "metrics_names = ['loss', 'acc'] \n",
        "# train_loss.reset_states()\n",
        "# train_accuracy.reset_states()\n",
        "for epoch in range(300, EPOCHS):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,EPOCHS))\n",
        "    pb_i = Progbar(train.shape[0], stateful_metrics=metrics_names)\n",
        " \n",
        "    # inp -> question, tar -> answer\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        \n",
        "        values=[('loss',train_loss.result()), ('acc',train_accuracy.result())]\n",
        "        \n",
        "        pb_i.add(batch_size, values=values) \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fe7b10",
      "metadata": {
        "id": "58fe7b10"
      },
      "source": [
        "# EPOCH:350, I think its enough, let's check what type of REPLIES that CHATBOT is generating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578270c4",
      "metadata": {
        "id": "578270c4"
      },
      "outputs": [],
      "source": [
        "# joblib.dump(tokenizer_q, \"tokenizer_q\")\n",
        "# joblib.dump(tokenizer_a, \"tokenizer_a\")\n",
        "# transformer.save_weights('transformer_model/weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c93b48c",
      "metadata": {
        "id": "5c93b48c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    sentence = sentence.split(\" \")\n",
        "    predicted_sentence = predicted_sentence.split(\" \")\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention,tokenizer_q, tokenizer_a, sentence, result, layer):\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence = tokenizer_q.encode(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "    #(1, 8, 5, 4) --> (8, 5, 4)\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights [:-1, :]\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "        fontdict = {'fontsize': 10}\n",
        "        \n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)-1))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        x = ['<start>']+[tokenizer_q.decode([i]) for i in sentence]+['<end>']\n",
        "        y = [tokenizer_a.decode([i]) for i in result if i < tokenizer_a.vocab_size]\n",
        "        ax.set_xticklabels([''] + x, fontdict=fontdict, rotation=90)\n",
        "        ax.set_yticklabels([''] + y, fontdict=fontdict)\n",
        "\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "MAX_LENGTH = 27 \n",
        "\n",
        "def evaluate(inp_sentence, model,  tokenizer_q, tokenizer_a):\n",
        "    start_token = [tokenizer_q.vocab_size]\n",
        "    end_token = [tokenizer_q.vocab_size + 1]\n",
        "\n",
        "    # All questions has the start and end token\n",
        "    inp_sentence = start_token + tokenizer_q.encode(inp_sentence) + end_token\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # 'answers' start token : 27358\n",
        "    decoder_input = [tokenizer_a.vocab_size]\n",
        "    decoder_input = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = model(encoder_input, \n",
        "                                                     decoder_input,\n",
        "                                                     False,\n",
        "                                                     enc_padding_mask,\n",
        "                                                     combined_mask,\n",
        "                                                     dec_padding_mask)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer_a.vocab_size+1:\n",
        "            print(f\"=============\\nGot end token\\n=============\")\n",
        "            return tf.squeeze(decoder_input, axis=0), attention_weights\n",
        "\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(decoder_input, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3aad84",
      "metadata": {
        "id": "4f3aad84"
      },
      "outputs": [],
      "source": [
        "def reply(sentence, transformer,  tokenizer_q, tokenizer_a, plot=''):\n",
        "    result, attention_weights = evaluate(sentence, transformer,  tokenizer_q, tokenizer_a)\n",
        "#     print(\"Attention_Blocks:\", list(attention_weights.keys()))\n",
        "    predicted_sentence = tokenizer_a.decode([i for i in result \n",
        "                                            if i < tokenizer_a.vocab_size])  \n",
        "  \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights,tokenizer_q, tokenizer_a, sentence, result, plot)\n",
        "    return sentence, predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b5f1253",
      "metadata": {
        "id": "1b5f1253",
        "outputId": "eccbeb77-5c2a-4cd8-8420-6e00bc5344ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Attention_Blocks: ['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2']\n",
            "Input: i was told ten thousand in each pack\n",
            "Predicted translation: you did not count it\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHgAAAHTCAYAAABcEa/JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABWBElEQVR4nO3deZhldX3v+/e35wm6AVsRIrbggIiA2g444TxrgmI0l6PB4ZBochHvURNjbswxxyRqcjyKJyYYZz3GOVFzHBIuglOCTAIiDlE4zsxTd9ND9ff+sVdj9VBVe1Prt/b+rXq/nqefqtq167N+tWvvT6/61tprR2YiSZIkSZKkei0a9wIkSZIkSZI0Pw54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyS8a9AGkuEXEpkDN9PjOP6XA5khYAe0dSl+wcSV2yc/rLAY9q8Izm7e81bz/YvD0Z2Nz9ciQtAPaOpC7ZOZK6ZOf0VGTOOLiTJkpEfC0zHzHXZZLUFntHUpfsHEldsnP6x3PwqCarI+KRuz6IiIcDq8e4Hkn9Z+9I6pKdI6lLdk7P+BQt1eQlwHsiYm3z8Y3Ai8e3HEkLgL0jqUt2jqQu2Tk941O0VJ2I2J/Bffemca9F0sJg70jqkp0jqUt2Tn844FE1ImI58BxgA9OOPsvMN4xrTZL6zd6R1CU7R1KX7Jz+8Slaqsk/ATcBFwBbx7wWSQuDvSOpS3aOpC7ZOT3T6yN4IiKATwOvzczvjHs9mp+IuCwzjx73OqTZ2Dv9Yu9o0tk5/WLnaNLZOf1i5/RP319F60nARuCl416IWvH1iLj/uBchzcHe6Rd7R5POzukXO0eTzs7pFzunZ/p+BM/HgPcAbweOyswdY16S5iEiLgfuCfyIwSGEAWRmHjPWhUnT2Dv9Yu9o0tk5/WLnaNLZOf1i5/RPb8/BExF3Au6XmV+IiH8FTgQ+PuZlaX6eOu4FSLOxd3rJ3tHEsnN6yc7RxLJzesnO6Zk+P0XrhcBHmvffC7xkjGtRCzLzqsy8CtgC5LR/6lBEnBgRa8a9jgll7/SMvTN+ds6s7JyesXPGz86ZlZ3TM3bOZGizd/o84HkRg+IhM78J3DUi7jbeJWk+IuJZEfF9BocQngNcCXx+rItaYCLiCOBjwH8a91omlL3TM/bOeNk5c7JzesbOGS87Z052Ts/YOePXdu/0csATEeuAd2TmT6dd/CrgTuNZkVryZ8DDgO9l5j2AxwNfG++SFpwXA29q3moae6e37J3xsnNmYOf0lp0zXnbODOyc3rJzxq/V3unlgCczbwQu2+OyfwFWjWVBasv2zLwOWBQRizLzbOC4Ma9pwYiIxcBzGRTQTRFx7JiXNFHsnd6yd8bEzpmdndNbds6Y2Dmzs3N6y84ZoxK908sBT+OMIS9TPW5snpt4LvDhiHgb4Jn7u/M04OuZeQuDV0/w5TH3Zu/0j70zPnbO3Oyc/rFzxsfOmZud0z92zni13ju9e5n0iDgeeDhwOvDWaZ/aHzgxM53GVyoiVjM4Adgi4GRgLfDhZuqswiLiH4G/zsyvRMQK4NvAfTNz23hXNn72Tn/ZO+Nj58zMzukvO2d87JyZ2Tn9ZeeMV4ne6eMRPMuANQxeAn6/af9uBk4a47rUkszcAXyDwUnAbh7vahaG5nnX6zLzKwCZeRvwCeBx41zXBLF3es7e6ZadMyc7p+fsnG7ZOXOyc3rOzuleqd7p3RE8cPtz2T6amRZOj0TEBcCjgAOAfwPOBzZn5sljXZiEvdNX9o4mlZ3TT3aOJpWd0092Tv8sGfcCSsjMqYg4cNzrUOsiMzdHxEuAMzLzzRFx0bgX1XcR8cDZPp+ZF3a1lklm7/SWvdMxO2c4dk5v2Tkds3OGY+f0lp0zBiV7p5cDnsZFEfEZ4OPApl0XZuanxrckzVM0zwE+GXhJc1mf78OT4q+btyuAjcC3gACOAf4deOSY1jWJ7J3+sXe6Z+cMz87pHzune3bO8Oyc/rFzxqNY7/T5h3cgcB27P4ctAQuoXqcDrwU+nZnfjojDgbPHu6TRRMShwN2Z9tjLzHPHt6K5ZeZjASLiH4BTM/PS5uOjgVeNc20TyN7pn9OpuHfsnN6zc/rndOycTtk5I7Fz+ud0Ku4csHf21Mtz8EiTKCLeBDwPuByYai7OzHxWS/kbMvPKPS57cGZ+s6X8izPzuLkukzQZ7BxJXSrdOc02ivWOnSPVx32dfWT2dcDTvMzYS4D7MTj0CYDMfPHYFqV5iYizGfyVYDeZWcUrHETEd4FjMnNrofwLgWdm5k+bj08A3pGZ928p/yMMDsf9EIOfw38C1mTmb7WR3wf2Tv/U3Dt2Tv/ZOf1j58y5jWK9Y+fMzc7pn5o7B9zX2Zc+P0Xrg8AVwJOBNzB4XuF3xroizdf0w9VWAM8BdoxpLXfED4GlQKkdn98B/jEingk8EPhz4Gkt5r8IeBnwiubjc4F3tpjfB/ZO/9TcO3ZO/9k5/WPnzK5k79g5c7Nz+qfmzgH3dfbS5yN4LsrMB0TEJZl5TEQsBb5YyzRSw4mIczLzhHGvYzYRcQaDieyhwLHAWUwrocw8rcVtHQ/8HXAb8PTMvKatbM3N3lkYJr137JyFw85ZGOycvbZn74yJnbMwTHrngPs6s+nzETzbm7c3Nicr+gWwYXzLGV5EvBn4b8AW4AsM7rSnZ+aH5pFZ/UtA7vHSjIuABwEHj2k5ozi/eXsB8Jm2wyPis+x+aOUq4Cbg3RFBi89BfQTwp+x9ErPD28jviSp7x86ZWaW9Y+csHHbO7rnV946ds29d9I6dMxQ7Z/dcO2d83NeZKbPHR/C8FPgkcH/gfcAa4P/NzL8b57qGsevEShFxIvAbwCuBszPz2Hlk7job+j5fii0zJ/4lICPiRwweaMHg0MEfAW/IzK+OdWFDiojVwG2ZOdV8vBhYnpmb55k764Q9M8+ZT/607VzB4L54Ab86iRmZeV0b+X1Qa+/YOTOruXfsnP6zc/bKrb537JwZs4v3jp0zNztnr1w7Z8zc19lbn4/gOSszb2DwPLbDASLiHuNd0tCWNm+fBnwkM6+PiHkFZg9eAjIza/n5zeQs4AnArc3HK4EvAQ+fT+iugmnu3z/PzNuaj1cCd5lP9h5uyszPt5jXR7X2jp0zg8p7x87pPztnmj70jp2zbx31jp0zNztnGjtnIrivs4c+D3g+yeBESNN9gsFhZ5Pus800bwvw8ohYz+A5f204clf5AGTmZRFxXEvZRMRyBifn2sDuh5m9oYXspQxOQvXo5qIvA3+Xmdtn/KLJsiIzd5UPmXlrRKxqMf/j7F5mU81lD24p/+yIeAvwKXZ/juvEH37aoVp7x86ZOb/m3rFz+s/O2bdivWPnzKp050DZ3rFz5mbn7Fu1+zqVdw64r7OX3g14IuJIBi/dtzYinj3tU/sz7eX8Jllm/mFEvAm4OTOnImIT8OstxX8nIv6e3V+Krc2z3/8Tg+cnXkD7ZzN/J4MJ/N80H7+gueylLW+nlE0R8cBdD9iIeBCD/2jasiQzt+36IDO3RcSyFvMf2rzdOO2yBBb8ifVq7x07Z1Y1946d01N2zpxK9o6dM7PSnQNle8fOmYGdM6ea93Vq7hxwX2cvvRvwAPcBngGsA5457fJbgP88jgXdQYcCT4yI6aX5gRZyS78E5K9l5lNazJvuwXs8V/b/i4hvFdpWCacDH4+InzUf3xV4Xov510TEszLzMwAR8evAtW2F7zoMVfvUh96xc/at5t45HTunr+yc2ZXsHTtnZqdTtnOgYO/YObOyc2ZX875OzZ0D7uvspc8nWT4+M78x7nXcERHxeuAxwFHA/waeCnw1M08a57qGERFnAmdMP0yxxewLgedm5n80Hx8OfCIzZz2D/SRpDoO8D4MTmV3R5uGPEXEE8GHgkCb/x8ALM/MHLeXfBfhz4JDMfGpEHAUcn5nvbiO/D2rtHTtn1vyqe8fO6Tc7p3t2zuxKdk6TX6x37Jy52Tnj4e9Xs3NfZ4/MHg94irwcXhci4lIG670oM49tfvB/n5nPnONL58qc8Yedmcfc0ew9tnM5cE8GZ2DfyuCBkG3kR8TjgfcCP2xy7w68KDPPnvULJ0hz0rWjmHY4a2a29deDXdtYw+CxfUvLuZ9ncPu/rrlfLmFwH71/m9upWa29Y+fMml9179g5/Wbn7DO3aO/YObPronOa7bTeO3bO3OycfeZWva9Te+eA+zp76uNTtHZ5Uma+JgYvh/cT4LnA2QyeGznptmTmzojYERH7A1fTnKl+Hp7RwrqG8dRSwZl5VkTci90ntCWe/17ETH89oL3DQ4mIpzN4jvSKaF4dIFs68SNwp8z8WES8tsndERFTc33RAlNr79g5M6i5d+ycBcHO2V0XvWPnzKCLzmm2U6p37Jy52Tm7q35fp+bOAfd19qXPA54iL4fXkfMjYh3wLgYn07oVOG8+gZl51a73m6n1rjN/n5eZV88ne8/tRMQjgXtl5ntjcJb6NW3lMzhL/wYG991jI6LIX4YKOYlf/fXgRbv+etBWeET8LbAKeGyTexLzvN/sYVNEHETzl4qIeBiDE77pV2rtHTtndrX2jp3Tf3bONF30jp0zq6KdA8V7x86Zm50zTY/2dWrtHHBfZy99forWXwK/weAQwocwOCnY5zLzobN82cSJiA3A/pl5SUt5vwm8hcFL4AXwKODVmfmJlvJfz+As4PfJzHtHxCHAxzPzES1kfxA4AriYwUvUweDwxNPmmz1tGw9n75cgbKXgIuK8zHxIRFzAoCRuAS7LzPu1lH9JZh4z7e0a4FOZ+aSW8h8InAEcDVwGrAdOauu+2Qd96B07Z6/8or1j58yab+fMwc6ZMbNY79TeOc02ivRO6c5ptlGsd+ycudk5M2ZWu69Tc+c02e7r7KG3R/Dk3i+Ht5l2Xw6vmIj4APAV4CuZeUXL8a9jcLb0q5ttrQf+FWilgIATgQcAFwJk5s8iYr+WsjcCR2WhqeRMBUd7h/gV+evBNLteEnBzU/zXAfdoKzwzL4yIE/jVIZzfzZZPnli7WnvHzplVsd6xc2Zn58zNzplRyd6ptnOgeO+U7hwo2Dt2ztzsnBnVvK9Tc+eA+zp76eWAJyJWMTiEbfpLvB3Er+5Uk+59wCOBM2JwJvOLgXMz820tZC/a45DB64BFLeTusi0zMyJ2HWa2usXsy4CDgZ+3mDld0YLLzJc37/5tRHyBlv96AHyuKbg3Myg4aOkQxT0eU99uLjssIqYy86dtbKN2lffO+7BzZlKyd+ycGdg5c7NzZlWyd2ruHCjYOx10DhTqHTtnbnbOrGre16m2c8B9nX3mFrqtxyoGL5V2BXBMZm5qLvsS8EeZef5YFzekiFjM4HmcjwV+l8HJwY5sIfctwDHAR5qLngdcmpmvmW92k/8q4F7AE4G/AF7M4Dm6b59H5mcZTHr3A45jMJW9/eRfmfmseSx5+nY+DpyWmUUKLiICOBk4PDPfEBGHAQdnZitT5ohYCbyMwWGhyeAvFe/MzNtayK7+MVVa7beRnbNXbvHesXNmza768dSF2m+jUp3TZBfrnZo7p9lOsd4p3TnNNor0Tu2Ppy7UfhvV2jlNvr9fzZzvvs6euX0c8ABExF8Bl2fme5of9D9l5gPGva5hRMRZwGrgGwzuRF/NFk/UFRHPZjDFDgbT60+3ld3kPxF4UpP/xcz8l3nmnTDb5zPznPnkT9vO2ZQtuHcCO4HHZeZ9I+IA4EuZ+eA5vnTY/I8xeN7prlcy+C1gXWb+Zkv51T6mulLrbWTn7DOzeO/YOXPmV/l46lKtt1Hpzmm2Uax3au2cZjvFeqd05zTbKNY7tT6eulTrbVR75zT5/n6173z3dfaUmb38BxzJ4HmWAH/MYHI49nUNufa3AucC/wL8KfA4YGVL2W8a5rJJzO9g7Sfs61+L+Rc2by+adtm3WszfK6vl/GofU139q/U2snPGk2/nzJlf5eOpy3+13kYlO6fJr3lfpNp9ndKdM1NeW9uo9fHU5b9ab6OaO6d0fs2d0+S7r7PHvzafGzhRsjmBVkTcm8Gk7YPjXdHwMvOVmfloBifUug54L3BjS/FP3MdlT20pu3R+0bVn5jn7+tdWPrC9OTx01/Nn1zOYOLflohi8tB5N/kOBr7UVXvNjqiu13kZ2znjy7ZzZ1fp46lKtt1HhzoGK90VK5xfundKdAwV7p9bHU5dqvY0q75zS+TV3Drivs5denmR5mnczOAnSJZl5w3zDIuKrmfnIiLiF5k6061MMXk5u//luo9nO7zN4nt+DgKuA9zA4nHA+mS8DXg4cHhHTTzy1Hy3cSUvmd7D2Tn6uwNuBTwN3jog3AicxmNS25aHACyPi/zQfHwZ8JyIuZfB9HNPCNlp9TPVUa7eRndPPfDtnJHbO3OycX+XWvC/Sh32d0p0D5XvHzplbdb9f1dg5pfN70jngvs5eensOHmDXmal/DjwnM/913OsZVkS8msFhhBdk5o6WMtcCBzA4MdcfTvvULZl5/STnl157lyLiSODxDMrtrMz8TovZd5/t85l5VQvbqPIx1aUabyM7p/v8rtg5/VfjbVSic5rcavdF7Jyh84v2To2Pp67VeBvV2Dml8/vSOeC+zl55fR7wSJIkSZIkLQS9PQePJEmSJEnSQrEgBjwRcar53WeXzq957aXza157X/jz7T679vya1146386ZW823f835Na+9dL5r7zd/vv3Mr3ntpfNrWfuCGPAApUu65nzX3s/8mtfeF/58u8+uPb/mtZfOt3PmVvPtX3N+zWsvne/a+82fbz/za1576fwq1r5QBjySJEmSJEm9VeVJlhevWp1L1x049PWnNm9i8arVw28gRlvP1KZNLF49XP6yNdtGCwe23bSFZWtXDnXdrbctHSl76tZNLF4z/G2zZNNoN86O2zaxZMVw+Tv2G+2+OHXLJhbvN/zal1+5eaT87WxlKcuHvv7WDatGyh91/fff/9qhr3vNdVOsP2jx0Nf/9i/WD31dgKktm1i8cvi133b1T67NzNE2MmHudODi3HC34R9fo/4Mvn/F2qGvu21qC8sWD9cJAMRoj9ttU5tZtni0+/Pw2SOuHWBq5/D5O7ewbNEI+YuH/xnB6LdNbt069HVH7RyAWL5s6Ovekds+lwx/+2zfvomlS4fvhYPvMfyLdNx0/Q7WHrhk6Ov/8qfbuen6HSP+bz5Zli1bnStWHDD09bdt28SyZSPs64zwuNq+YzNLl4zQCYtH+/vhyGvfOdr+wqj3zdg+NfR1t+3czLJFw9822/cfbT9tlP0ogKU3bBkpf9vO21i2aMXw6xlyfxRGXztAjnDXGTV/yW0j/F+yfRPLRrjP3HbbjWzbPuJO8oRZvGZ1LjlwhN+vbr2VxWvWDH39Fb/cPtJ6Rvn/dpT/qwC279jE0iUjdMLO4e87UHY/auT8UfcBd2xm2Sh9P2Ifj3zbLBp1/ZtYNsLPtmT2bXca7X456u/l236879+vht9bmiBL1x3Ihpf+P8Xyp5aXG3od9vCfFMsG+MH37lo0/07njXZHHcV1J4w+/BrFvU65oGj+9//rg4rmn/ekdxfLPvZNLy+WDXDZ//h/5v0SguO24W5LOe+LdyuW//Tjn1kse9QhxqhyxP98R3bLpnLZa/crlw1M/eDKovmL7z7rq3fO29RBw++8j+q/fOgjxbJP+/UfFcvuyooVB7Bx4+8Vy19y0/DDx1HtWDfaoHJUi24bfgBzRyz9xY3Fsn/5uEOKZQPc+ePfLpp/w1OPKpq/bU25/08O/O5txbK/ef7/LJbdlSUHHshd/+AVxfLv+9c/L5Y9tX74P5LdEYtuLXffAUYewowil5X9dT82l/u/BCBXjDYUnyTffcm6ovlXnfaqff5+5VO0JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirXyoAnIv4sIl4x7eM3RsRpMfCWiLgsIi6NiOc1n39MRHxu2vXfERGntLEWSf1n50jqmr0jqUt2jqQ7YklLOe8GPgW8LSIWAc8HHgI8GzgOOBa4E/DNiDj3jmwgIk4FTgVYsvaAFpYsqWLFOwd2753DDm2rLiVVqtN9neXL181/xZJq1mnnLD5g3fxXLGnsWjmCJzOvBK6LiAcATwIuyszrgEcCH8nMqcz8JXAO8OA7uI0zM3NjZm5cvGp1G8uWVKkuOqfZzu29s/6gxW0sXVKlut7XWbbMfR1pIev896s1a9pauqQxavNP0n8PnAIcDLynuSxmuO4Odh8urWhxHZIWBjtHUtfsHUldsnMkjaTNkyx/GngKgwnyF5vLzgWeFxGLI2I98GjgPOAq4KiIWB4Ra4HHt7gOSQuDnSOpa/aOpC7ZOZJG0toRPJm5LSLOBm7MzKnm4k8DxwPfAhJ4TWb+AiAiPgZcAnwfuKitdUhaGOwcSV2zdyR1yc6RNKrWBjzNyb8eBjx312WZmcCrm3+7yczXAK9pa/uSFhY7R1LX7B1JXbJzJI2qrZdJPwr4AXBWZn6/jUxJmomdI6lr9o6kLtk5ku6IVo7gyczLgcPbyJKkudg5krpm70jqkp0j6Y5o8yTLkiRJkiRJGgMHPJIkSZIkSZVzwCNJkiRJklS51l5Fq0uLdsCKa7NY/g3HTs19pTvoh5ccWiwb4ND7Xl00/9qrDy6WfcjBNxTLBlh03FFF81ftf1vR/Aee/7xi2Uu2lHs89cX3v7uOpz36xGL5Nx5/l2LZK6/eXiwb4Lr7LS+af/C/3VIs+8dP2K9YNsCv/cV/FM3/4QvK3W8Atm3YWiz7bU98WrHsX/7kQ8WyO3PLZhZ/+cJi8XH0kcWyc1EUywaYWll293XJrZuLZd9Y7mYH4KCbby6av+ngsn8bvvUe5fbB73RmwVcNzy3lsjuy4qdbuM8fXV4sP+9212LZm+62qlg2wNKblxXNX/Hjm4pl//zR64plA9zljG8Uzb/htx9WNP/ax5Xbz7nP7327WDbAVTNc7hE8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZUrPuCJiD+NiFc1778hIp6wj+s8JiI+V3otkvrPzpHUJTtHUtfsHUkzWdLlxjLzT7rcnqSFzc6R1CU7R1LX7B1J0xU5giciXhcR342IfwXuM+3y90XESc37T4mIKyLiq8CzS6xD0sJg50jqkp0jqWv2jqRhtH4ET0Q8CHg+8IAm/0Lggj2uswJ4F/A44AfAR4fIPRU4FWDpmgPaXbSkapXqnObrbu+dFUv2b2/RkqrVWeewqr1FS6paF79frYjV7S5a0liUOILnUcCnM3NzZt4MfGYf1zkS+FFmfj8zE/jQXKGZeWZmbszMjUtWWkCSblekc2D33lm2eGWLS5ZUsU46ZynLW1yypMoV//1qWaxoecmSxqHUSZazpetI0jDsHEldsnMkdc3ekTSnEgOec4ETI2JlROwHPHMf17kCuEdEHNF8/FsF1iFpYbBzJHXJzpHUNXtH0lBaPwdPZl4YER8FLgauAr6yj+vc1jzn858j4lrgq8DRba9FUv/ZOZK6ZOdI6pq9I2lYRV4mPTPfCLxxH5efMu39LzB4rqgkzYudI6lLdo6krtk7koZR6hw8kiRJkiRJ6ogDHkmSJEmSpMo54JEkSZIkSaqcAx5JkiRJkqTKOeCRJEmSJEmqXJFX0Spt51LYcnAUy7/3+28rlv2j31hdLBtg8z/epWj++qunimX/jxf+Q7FsgFff+eVF849cf2XR/DPu/o/Fsh93wWuKZffF1kMX84M/279Y/oZ3lOsdMstlA+sv3lk0f9vaZcWyD/vn64tlA2x5+oOL5t/tS1uK5sdUufvOd/7rQcWyb/uTKndvdnPA/bbznE9eXSz/U/cvdxst/W7lfz9cuaJY9BF/cF6xbICthTvnrv/j34vml3TfC8rd5y85uVh0Z+519K18/ot7vfp6a5586AOKZa+6onDnZNn9nKkot/67fPcHxbIBlhxc9nfPA97/b2Xz31duP+dNV5Zd+5fuvu/LK/8fWJIkSZIkSQ54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKjf2AU9EnBIRh4x7HZIWBjtHUtfsHUldsnOkhWvsAx7gFMACktSVU7BzJHXrFOwdSd05BTtHWpBaHfBExIaI+E5EvCsivh0RX4qIlc3njouIf4uISyLi0xFxQEScBGwEPhwRF++6riQNw86R1DV7R1KX7BxJoyhxBM+9gP+ZmfcDbgSe01z+AeAPMvMY4FLg9Zn5CeB84OTMPC4zt8wUGhGnRsT5EXH+1KZNBZYtqVJFOgf26J2b7R1Jtyu+r3PrDdvLfgeSalK8c665bqrsdyCpEyUGPD/KzIub9y8ANkTEWmBdZp7TXP5+4NGjhGbmmZm5MTM3Ll69ur3VSqpdkc6BPXpnf3tH0u2K7+usOWBpe6uVVLvinbP+oMXtrVbS2JQY8Gyd9v4UsKTANiRpFztHUtfsHUldsnMkDaWTkyxn5k3ADRHxqOaiFwC7ps23APt1sQ5JC4OdI6lr9o6kLtk5kvaly+nvbwN/GxGrgB8CL2ouf19z+Rbg+LnOiSFJQ7JzJHXN3pHUJTtH0m5aHfBk5pXA0dM+/qtp718MPGwfX/NJ4JNtrkPSwmDnSOqavSOpS3aOpFF08hQtSZIkSZIkleOAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq1+XLpLcml+9kx703F8v/8fY1xbJ3rJ4qlg2w+ZCyM7ubH7WtWPbJHzi9WDbAzsdl0fyrzr9n0fyn/69XF8ve/khfPXMuy3+8g3u96ppi+dc84e7FsrftH8WyB/lF41n7o53Fsq87+sBi2QCHnHlx0fz/84rjiuav+mW53rzva35YLPuGa8v9X9WVFYu2c+TynxXLzx13LpZdu6mpgvtqO8vuB1KuLpv8wusv6IgVNxTLXr5oR7Hsrvx4+ypO//nGYvmxuFg0WfIx24FYXO7GySxbCt8/7fCi+fd43dVF80v6y58+tfAW3rXPSz2CR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyYx/wRMTpEbFq3OuQtDDYOZK6Zu9I6pKdIy1cYx/wAKcDFpCkrpyOnSOpW6dj70jqzunYOdKCNOeAJyJeGBGXRMS3IuKDzWV3j4izmsvPiojDmsvfFxEnTfvaW5u3j4mIL0fEJyLiioj4cAycBhwCnB0RZ5f5FiXVxM6R1DV7R1KX7BxJpcw64ImI+wGvAx6XmccCr2g+9Q7gA5l5DPBh4O1DbOsBDKbJRwGHA4/IzLcDPwMem5mPnWMtp0bE+RFx/s5bNg2xOUm1maTOadZze+9s27ll5O9H0uSbpN6Z3jk3XTd1h74fSZNtUjtnyw1b79D3I2myzHUEz+OAT2TmtQCZeX1z+fHA/2re/yDwyCG2dV5m/iQzdwIXAxtGWWhmnpmZGzNz46L9Vo/ypZLqMTGd02z/9t5ZtmjlqF8uqQ4T0zvTO2ftQYtH+VJJ9ZjIzll5wPJRvlTShJprwBNADpGz6zo7dmVGRADLpl1n+lh4Clgy5BolLRx2jqSu2TuSumTnSCpmrgHPWcBvRsRBABFxYHP514HnN++fDHy1ef9K4EHN+78OLB1iDbcA+w25Xkn9ZudI6pq9I6lLdo6kYmYd8GTmt4E3AudExLeA/9586jTgRRFxCfACfvXc0XcBJ0TEecBDgWFOlnMm8HlPAibJzpHUNXtHUpfsHEklzXkYX2a+H3j/HpddyeD5o3te95fAw6Zd9Nrm8i8DX552vd+f9v4ZwBkjrVpSb9k5krpm70jqkp0jqZQ5XyZdkiRJkiRJk80BjyRJkiRJUuUc8EiSJEmSJFXOAY8kSZIkSVLlHPBIkiRJkiRVbs5X0ZpES28I7vrRZcXyrz26WDQvePRXy4UDn/joCUXz/+hBny2W/TcHPKZYNsC6168smv/D/xJF829cUm79d/uHpcWyAX5UNL0bhx55A2/87KeL5b/2vuUeu7GsXF8CZGbR/EXrDyqWvfbHPyuWDfCD9x9VNP+I3z6/aD65s1j0iy//XrHs7504zKsIT7b9Ah69olz+G8tFa4xWX/bzovk7iqaXdfjyXxbLXh7bi2V35c5LbuH/vtOXi+X/Ho8plh2LFxfLBsidZfdzYnG5Yy5ye9m13+Mzhf+/LbyPWdLT73RJ0fyPznC5R/BIkiRJkiRVzgGPJEmSJElS5RzwSJIkSZIkVc4BjyRJkiRJUuUc8EiSJEmSJFXOAY8kSZIkSVLlHPBIkiRJkiRVzgGPJEmSJElS5SZiwBMRX2/eboiI/2vc65HUb3aOpC7ZOZK6Zu9IC9NEDHgy8+HNuxsAC0hSUXaOpC7ZOZK6Zu9IC9NEDHgi4tbm3b8EHhURF0fEK8e5Jkn9ZedI6pKdI6lr9o60MC0Z9wL28IfAqzLzGeNeiKQFwc6R1CU7R1LX7B1pAZmII3iGERGnRsT5EXH+jq2bxr0cSQvA9N654fqd416OpJ6b3jnXXDc17uVI6rnpnXO9+zlSL1Qz4MnMMzNzY2ZuXLJ89biXI2kBmN47BxxYTV1KqtT0zll/0OJxL0dSz03vnAPdz5F6YdIeybcA+417EZIWDDtHUpfsHElds3ekBWTSBjyXADsi4lueBExSB+wcSV2ycyR1zd6RFpCJOMlyZq5p3m4HHj/m5UjqOTtHUpfsHElds3ekhWnSjuCRJEmSJEnSiBzwSJIkSZIkVc4BjyRJkiRJUuUc8EiSJEmSJFXOAY8kSZIkSVLlHPBIkiRJkiRVLjJz3GsYWURcA1w1wpfcCbi20HJqz3ft/cyftLXfPTPXl1pMFyasdybt5zsp2bXn17z20vl2ztwm6fZfSPk1r710/kJau53Tvkn6+S6k/JrXXjp/0ta+z96pcsAzqog4PzM3mt9tdun8mtdeOr/mtfeFP9/us2vPr3ntpfPtnLnVfPvXnF/z2kvnu/Z+8+fbz/ya1146v5a1+xQtSZIkSZKkyjngkSRJkiRJqtxCGfCcaf5Yskvn17z20vk1r70v/Pl2n117fs1rL51v58yt5tu/5vya114637X3mz/ffubXvPbS+VWsfUGcg0fdiIhbM3PNtI9PATZm5u+3kP1l4FWZef4el/8+cDpwBLA+M0ue+ErSBBlT53wY2AhsB84Dficzt893e5Im35g6590MOieA7wGnZOat892epDqMo3emff4M4EXTt6/Jt1CO4FF/fQ14AqOd9V+S7qgPA0cC9wdWAi8d73Ik9dwrM/PYzDwG+D/AvH+pk6S5RMRGYN2416HROeBRJyJifUR8MiK+2fx7RHP5QyLi6xFxUfP2Ps3lKyPiHyLikoj4KINfpPaSmRdl5pXdfSeSalCwc/53NhgcwfNrnX1TkiZWwc65ubl+NNfx0HtJQLneiYjFwFuA13T2zag1S8a9APXKyoi4eNrHBwKfad5/G/DWzPxqRBwGfBG4L3AF8OjM3BERTwD+HHgO8DJgc2YeExHHABd29U1IqsbYOicilgIvAF7R5jckaaKNpXMi4r3A04DLgf/S8vckabKNo3d+H/hMZv58MFtWTRzwqE1bMvO4XR/seo5o8+ETgKOmlcT+EbEfsBZ4f0Tci8FfpZY2n3808HaAzLwkIi4pvnpJtRln5/wNcG5mfqWF70NSHcbSOZn5ouYv6mcAzwPe29Y3JGniddo7EXEI8FzgMW1/I+qGAx51ZRFwfGZumX5hc/KuszPzxIjYAHx52qc9DFnSHVWscyLi9cB64HfaWaqkHii6n5OZU81TKl6NAx5JAyV65wHAPYEfNIOjVRHxg8y8Z2urVlGeg0dd+RLTTgwYEcc1764Fftq8f8q0658LnNxc92jgmOIrlNQnRTonIl4KPBn4rczc2eqKJdWs9c6JgXvueh94JoOnXkgSFOidzPznzDw4Mzdk5gYGT+lyuFMRBzzqymnAxuakXpcDv9tc/mbgLyLia8Diadd/J7CmOXTwNQxOZrqXiDgtIn7C4ESnl0TE3xf7DiTVpEjnAH8L3AX4RkRcHBF/Umb5kipTonOCwdMsLgUuBe4KvKHUNyCpOqX2dVSxGLwQiCRJkiRJkmrlETySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVLkl416ANJeIuBTImT6fmcd0uBxJC4C9I6lLdo6kLtk5/eWARzV4RvP295q3H2zengxs7n45khYAe0dSl+wcSV2yc3oqMmcc3EkTJSK+lpmPmOsySWqLvSOpS3aOpC7ZOf3jOXhUk9UR8chdH0TEw4HVY1yPpP6zdyR1yc6R1CU7p2d8ipZq8hLgPRGxtvn4RuDF41uOpAXA3pHUJTtHUpfsnJ7xKVqqTkTsz+C+e9O41yJpYbB3JHXJzpHUJTunPxzwqBoRsRx4DrCBaUefZeYbxrUmSf1m70jqkp0jqUt2Tv/4FC3V5J+Am4ALgK1jXoukhcHekdQlO0dSl+ycnun1ETwREcCngddm5nfGvR7NT0RclplHj3sd0mzsnX6xdzTp7Jx+sXM06eycfrFz+qfvr6L1JGAj8NJxL0St+HpE3H/ci5DmYO/0i72jSWfn9Iudo0ln5/SLndMzfT+C52PAe4C3A0dl5o4xL0nzEBGXA/cEfsTgEMIAMjOPGevCpGnsnX6xdzTp7Jx+sXM06eycfrFz+qe35+CJiDsB98vML0TEvwInAh8f87I0P08d9wKk2dg7vWTvaGLZOb1k52hi2Tm9ZOf0TJ+fovVC4CPN++8FXjLGtagFmXlVZl4FbAFy2j91KCJOjIg1417HhLJ3esbeGT87Z1Z2Ts/YOeNn58zKzukZO2cytNk7fR7wvIhB8ZCZ3wTuGhF3G++SNB8R8ayI+D6DQwjPAa4EPj/WRS0wEXEE8DHgP417LRPK3ukZe2e87Jw52Tk9Y+eMl50zJzunZ+yc8Wu7d3o54ImIdcA7MvOn0y5+FXCn8axILfkz4GHA9zLzHsDjga+Nd0kLzouBNzVvNY2901v2znjZOTOwc3rLzhkvO2cGdk5v2Tnj12rv9HLAk5k3Apftcdm/AKvGsiC1ZXtmXgcsiohFmXk2cNyY17RgRMRi4LkMCuimiDh2zEuaKPZOb9k7Y2LnzM7O6S07Z0zsnNnZOb1l54xRid7p5YCnccaQl6keNzbPTTwX+HBEvA3wzP3deRrw9cy8hcGrJ/jymHuzd/rH3hkfO2dudk7/2DnjY+fMzc7pHztnvFrvnd69THpEHA88HDgdeOu0T+0PnJiZTuMrFRGrGZwAbBFwMrAW+HAzdVZhEfGPwF9n5lciYgXwbeC+mbltvCsbP3unv+yd8bFzZmbn9JedMz52zszsnP6yc8arRO/08QieZcAaBi8Bv9+0fzcDJ41xXWpJZu4AvsHgJGA3j3c1C0PzvOt1mfkVgMy8DfgE8LhxrmuC2Ds9Z+90y86Zk53Tc3ZOt+ycOdk5PWfndK9U7/TuCB64/blsH81MC6dHIuIC4FHAAcC/AecDmzPz5LEuTMLe6St7R5PKzuknO0eTys7pJzunf5aMewElZOZURBw47nWodZGZmyPiJcAZmfnmiLho3Ivqu4h44Gyfz8wLu1rLJLN3esve6ZidMxw7p7fsnI7ZOcOxc3rLzhmDkr3TywFP46KI+AzwcWDTrgsz81PjW5LmKZrnAJ8MvKS5rM/34Unx183bFcBG4FtAAMcA/w48ckzrmkT2Tv/YO92zc4Zn5/SPndM9O2d4dk7/2DnjUax3+vzDOxC4jt2fw5aABVSv04HXAp/OzG9HxOHA2eNd0mgi4lDg7kx77GXmueNb0dwy87EAEfEPwKmZeWnz8dHAq8a5tglk7/TP6VTcO3ZO79k5/XM6dk6n7JyR2Dn9czoVdw7YO3vq5Tl4pEkUEW8CngdcDkw1F2dmPqul/A2ZeeUelz04M7/ZUv7FmXncXJdJmgx2jqQule6cZhvFesfOkerjvs4+Mvs64GleZuwlwP0YHPoEQGa+eGyL0rxExNkM/kqwm8ys4hUOIuK7wDGZubVQ/oXAMzPzp83HJwDvyMz7t5T/EQaH436Iwc/hPwFrMvO32sjvA3unf2ruHTun/+yc/rFz5txGsd6xc+Zm5/RPzZ0D7uvsS5+fovVB4ArgycAbGDyv8DtjXZHma/rhaiuA5wA7xrSWO+KHwFKg1I7P7wD/GBHPBB4I/DnwtBbzXwS8DHhF8/G5wDtbzO8De6d/au4dO6f/7Jz+sXNmV7J37Jy52Tn9U3PngPs6e+nzETwXZeYDIuKSzDwmIpYCX6xlGqnhRMQ5mXnCuNcxm4g4g8FE9lDgWOAsppVQZp7W4raOB/4OuA14emZe01a25mbvLAyT3jt2zsJh5ywMds5e27N3xsTOWRgmvXPAfZ3Z9PkInu3N2xubkxX9AtgwvuUMLyLeDPw3YAvwBQZ32tMz80PzyKz+JSD3eGnGRcCDgIPHtJxRnN+8vQD4TNvhEfFZdj+0chVwE/DuiKDF56A+AvhT9j6J2eFt5PdElb1j58ys0t6xcxYOO2f33Op7x87Zty56x84Zip2ze66dMz7u68yU2eMjeF4KfBK4P/A+YA3w/2bm341zXcPYdWKliDgR+A3glcDZmXnsPDJ3nQ19ny/FlpkT/xKQEfEjBg+0YHDo4I+AN2TmV8e6sCFFxGrgtsycaj5eDCzPzM3zzJ11wp6Z58wnf9p2rmBwX7yAX53EjMy8ro38Pqi1d+ycmdXcO3ZO/9k5e+VW3zt2zozZxXvHzpmbnbNXrp0zZu7r7K3PR/CclZk3MHge2+EAEXGP8S5paEubt08DPpKZ10fEvAKzBy8BmZm1/PxmchbwBODW5uOVwJeAh88ndFfBNPfvn2fmbc3HK4G7zCd7Dzdl5udbzOujWnvHzplB5b1j5/SfnTNNH3rHztm3jnrHzpmbnTONnTMR3NfZQ58HPJ9kcCKk6T7B4LCzSffZZpq3BXh5RKxn8Jy/Nhy5q3wAMvOyiDiupWwiYjmDk3NtYPfDzN7QQvZSBiehenRz0ZeBv8vM7TN+0WRZkZm7yofMvDUiVrWY/3F2L7Op5rIHt5R/dkS8BfgUuz/HdeIPP+1Qrb1j58ycX3Pv2Dn9Z+fsW7HesXNmVbpzoGzv2Dlzs3P2rdp9nco7B9zX2UvvBjwRcSSDl+5bGxHPnvap/Zn2cn6TLDP/MCLeBNycmVMRsQn49ZbivxMRf8/uL8XW5tnv/4nB8xMvoP2zmb+TwQT+b5qPX9Bc9tKWt1PKpoh44K4HbEQ8iMF/NG1Zkpnbdn2QmdsiYlmL+Q9t3m6cdlkCC/7EerX3jp0zq5p7x87pKTtnTiV7x86ZWenOgbK9Y+fMwM6ZU837OjV3Drivs5feDXiA+wDPANYBz5x2+S3Afx7Hgu6gQ4EnRsT00vxAC7mlXwLy1zLzKS3mTffgPZ4r+/9FxLcKbauE04GPR8TPmo/vCjyvxfxrIuJZmfkZgIj4deDatsJ3HYaqfepD79g5+1Zz75yOndNXds7sSvaOnTOz0ynbOVCwd+ycWdk5s6t5X6fmzgH3dfbS55MsH5+Z3xj3Ou6IiHg98BjgKOB/A08FvpqZJ41zXcOIiDOBM6Yfpthi9oXAczPzP5qPDwc+kZmznsF+kjSHQd6HwYnMrmjz8MeIOAL4MHBIk/9j4IWZ+YOW8u8C/DlwSGY+NSKOAo7PzHe3kd8HtfaOnTNrftW9Y+f0m53TPTtndiU7p8kv1jt2ztzsnPHw96vZua+zR2aPBzxFXg6vCxFxKYP1XpSZxzY/+L/PzGfO8aVzZc74w87MY+5o9h7buRy4J4MzsG9l8EDINvIj4vHAe4EfNrl3B16UmWfP+oUTpDnp2lFMO5w1M9v668Gubaxh8Ni+peXczzO4/V/X3C+XMLiP3r/N7dSs1t6xc2bNr7p37Jx+s3P2mVu0d+yc2XXROc12Wu8dO2duds4+c6ve16m9c8B9nT318SlauzwpM18Tg5fD+wnwXOBsBs+NnHRbMnNnROyIiP2Bq2nOVD8Pz2hhXcN4aqngzDwrIu7F7hPaEs9/L2Kmvx7Q3uGhRMTTGTxHekU0rw6QLZ34EbhTZn4sIl7b5O6IiKm5vmiBqbV37JwZ1Nw7ds6CYOfsrovesXNm0EXnNNsp1Tt2ztzsnN1Vv69Tc+eA+zr70ucBT5GXw+vI+RGxDngXg5Np3QqcN5/AzLxq1/vN1HrXmb/Py8yr55O953Yi4pHAvTLzvTE4S/2atvIZnKV/A4P77rERUeQvQ4WcxK/+evCiXX89aCs8Iv4WWAU8tsk9iXneb/awKSIOovlLRUQ8jMEJ3/QrtfaOnTO7WnvHzuk/O2eaLnrHzplV0c6B4r1j58zNzpmmR/s6tXYOuK+zlz4/Resvgd9gcAjhQxicFOxzmfnQWb5s4kTEBmD/zLykpbzfBN7C4CXwAngU8OrM/ERL+a9ncBbw+2TmvSPiEODjmfmIFrI/CBwBXMzgJepgcHjiafPNnraNh7P3SxC2UnARcV5mPiQiLmBQErcAl2Xm/VrKvyQzj5n2dg3wqcx8Ukv5DwTOAI4GLgPWAye1dd/sgz70jp2zV37R3rFzZs23c+Zg58yYWax3au+cZhtFeqd05zTbKNY7ds7c7JwZM6vd16m5c5ps93X20NsjeHLvl8PbTLsvh1dMRHwA+Arwlcy8ouX41zE4W/rVzbbWA/8KtFJAwInAA4ALATLzZxGxX0vZG4GjstBUcqaCo71D/Ir89WCaXS8JuLkp/uuAe7QVnpkXRsQJ/OoQzu9myydPrF2tvWPnzKpY79g5s7Nz5mbnzKhk71TbOVC8d0p3DhTsHTtnbnbOjGre16m5c8B9nb30csATEasYHMI2/SXeDuJXd6pJ9z7gkcAZMTiT+cXAuZn5thayF+1xyOB1wKIWcnfZlpkZEbsOM1vdYvZlwMHAz1vMnK5owWXmy5t3/zYivkDLfz0APtcU3JsZFBy0dIjiHo+pbzeXHRYRU5n50za2UbvKe+d92DkzKdk7ds4M7Jy52TmzKtk7NXcOFOydDjoHCvWOnTM3O2dWNe/rVNs54L7OPnML3dZjFYOXSrsCOCYzNzWXfQn4o8w8f6yLG1JELGbwPM7HAr/L4ORgR7aQ+xbgGOAjzUXPAy7NzNfMN7vJfxVwL+CJwF8AL2bwHN23zyPzswwmvfsBxzGYyt5+8q/MfNY8ljx9Ox8HTsvMIgUXEQGcDByemW+IiMOAgzOzlSlzRKwEXsbgsNBk8JeKd2bmbS1kV/+YKq3228jO2Su3eO/YObNmV/146kLtt1Gpzmmyi/VOzZ3TbKdY75TunGYbRXqn9sdTF2q/jWrtnCbf369mzndfZ8/cPg54ACLir4DLM/M9zQ/6nzLzAeNe1zAi4ixgNfANBneir2aLJ+qKiGczmGIHg+n1p9vKbvKfCDypyf9iZv7LPPNOmO3zmXnOfPKnbedsyhbcO4GdwOMy874RcQDwpcx88BxfOmz+xxg873TXKxn8FrAuM3+zpfxqH1NdqfU2snP2mVm8d+ycOfOrfDx1qdbbqHTnNNso1ju1dk6znWK9U7pzmm0U651aH09dqvU2qr1zmnx/v9p3vvs6e8rMXv4DjmTwPEuAP2YwORz7uoZc+1uBc4F/Af4UeBywsqXsNw1z2STmd7D2E/b1r8X8C5u3F0277Fst5u+V1XJ+tY+prv7VehvZOePJt3PmzK/y8dTlv1pvo5Kd0+TXvC9S7b5O6c6ZKa+tbdT6eOryX623Uc2dUzq/5s5p8t3X2eNfm88NnCjZnEArIu7NYNL2wfGuaHiZ+crMfDSDE2pdB7wXuLGl+Cfu47KntpRdOr/o2jPznH39aysf2N4cHrrr+bPrGUyc23JRDF5ajyb/ocDX2gqv+THVlVpvIztnPPl2zuxqfTx1qdbbqHDnQMX7IqXzC/dO6c6Bgr1T6+OpS7XeRpV3Tun8mjsH3NfZSy9PsjzNuxmcBOmSzLxhvmER8dXMfGRE3EJzJ9r1KQYvJ7f/fLfRbOf3GTzP70HAVcB7GBxOOJ/MlwEvBw6PiOknntqPFu6kJfM7WHsnP1fg7cCngTtHxBuBkxhMatvyUOCFEfF/mo8PA74TEZcy+D6OaWEbrT6meqq128jO6We+nTMSO2duds6vcmveF+nDvk7pzoHyvWPnzK26369q7JzS+T3pHHBfZy+9PQcPsOvM1D8HnpOZ/zru9QwrIl7N4DDCCzJzR0uZa4EDGJyY6w+nfeqWzLx+kvNLr71LEXEk8HgG5XZWZn6nxey7z/b5zLyqhW1U+ZjqUo23kZ3TfX5X7Jz+q/E2KtE5TW61+yJ2ztD5RXunxsdT12q8jWrsnNL5fekccF9nr7w+D3gkSZIkSZIWgt6eg0eSJEmSJGmhWBADnog41fzus0vn17z20vk1r70v/Pl2n117fs1rL51v58yt5tu/5vya114637X3mz/ffubXvPbS+bWsfUEMeIDSJV1zvmvvZ37Na+8Lf77dZ9eeX/PaS+fbOXOr+favOb/mtZfOd+395s+3n/k1r710fhVrXygDHkmSJEmSpN6q8iTLy5auzhXL1w19/e3bN7F06eqhr79z2Whzr+1bb2Xp8jXDXfkO3Nyj5MeIP8/t2zaxdNnwt82i20Y78fy2qS0sW7xyuOuuWzpS9tSWTSxeOfzal/5y00j529nKUpYPf/27DL8WgKnNm1i8aoSvWTM1fPbNm1m8/6qhr7/05zH8OoDtOzaxdMnwa79l88+uzcz1I21kwixevTqXrjtw6OtPbdrE4tXD30bLr9029HVHeVwBsGi0Tts2tZlli4e//zBC74y8doCpncPn79zCskUj5C9ZPNJSRr5tJiT7jubvXDr87TPq/7Xb9xu+d0Z9PG2/8XqmNm0ardgmzLJYnitihO85t7I0hv8/K1asGPq623ZsZtmS4e87uWjE/1NGvO+MatT82DpCH++8jWWLhr8tt955+OvC6Pf9ZT8rvK9z8PBrGXk/B9i5bPjrTt16K4vXDLn/DSz/8fC3zaiPp9tyE9tya92ds3R1rli2bujrj7ovOOphBdu2b2JZoV4YOTtG+9Fu27aJZSP8fhVbt4+WP8K+1Pa1IzyogB1bNrFklN+vbtg6Uv6o+2nb1w3/OATYcdsmlqwoc78ZNXvJ5uF/dwPYvmMzS0f4v/aWzT/f5+9XS0ba6oRYsXwdDz3md4vlbzp0xF8+RhDD/55yhyzaXnZgt/qKa4pl//g37losG+CQv/p60fxfnPzwovmccEOx6Lu+sWwV/Mt5r5/3SwiO29J1B3LYy15ZLP+Id/24WHauKddpAOwY7T+wkd14S7nsOw8/tLtDKvwjynS3HbJfseyfnjDaTucofvw3by2W3ZUVsZqHLXlysfw48t7FsncuG21wOqpcXPYA9CXfK9fH//F79ymWDbDhj79RNP9nv112X2fTYeX+P7n36RcUy/63HV8slt2VFcvW8bCjf6dY/tSKgvuahf+vzSVlO2f5f1xdLPvnz7hbsWyAgz/1H0Xzr37mEUXzs+CPdv2FBfdfgX/55p/u8/crn6IlSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlWtlwBMRfxYRr5j28Rsj4rQYeEtEXBYRl0bE85rPPyYiPjft+u+IiFPaWIuk/rNzJHXN3pHUJTtH0h2xpKWcdwOfAt4WEYuA5wMPAZ4NHAccC9wJ+GZEnHtHNhARpwKnAqxYtraFJUuqWPHOgd17Z8naA+a5ZEmV63Zfh1UtLFlSxfz9StLIWhnwZOaVEXFdRDwAuAtwUWZeFxGPBD6SmVPALyPiHODBwM13YBtnAmcC7L/m0Gxj3ZLq1EXnNNu5vXdWHHo3e0dawDrf11l0oJ0jLWCdd85qf7+S+qCtI3gA/h44BTgYeE9zWcxw3R3s/vSwFS2uQ9LCYOdI6pq9I6lLdo6kkbR5kuVPA09hMEH+YnPZucDzImJxRKwHHg2cB1wFHBURyyNiLfD4FtchaWGwcyR1zd6R1CU7R9JIWjuCJzO3RcTZwI3NIYMwKKXjgW8BCbwmM38BEBEfAy4Bvg9c1NY6JC0Mdo6krtk7krpk50gaVWsDnubkXw8DnrvrssxM4NXNv91k5muA17S1fUkLi50jqWv2jqQu2TmSRtXWy6QfBfwAOCszv99GpiTNxM6R1DV7R1KX7BxJd0Rbr6J1OXB4G1mSNBc7R1LX7B1JXbJzJN0RbZ5kWZIkSZIkSWPggEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcq29THqX7n3EdXzxUx8olv/0Bz2lWDbLl5XLBvKWW4vmx35rimUf9vEfF8sGyKPuXTT/0M/+vGh+vrNc/tu+e1axbID7HlY0vhPLr9nGEX93VbH8Xz7t7sWyV10zVSwbYOnNZfNvfuRdimXv99PtxbIBVl5ethe23qvcbQOw4sc3Fcu+599uLpZ99TXbimV35d7338wXvnh+sfwnH7KjWHZpUTi/ZKNt+ONvFEyHRatWFc0/5M1fL5pf0qv/49Ji2b/3rHJ91pVcEmw9aEWx/Nixs1j2spvKdv7U4rKtM3WXdcWyV/+y3O0OECuWF80vbf+ryu0Hbls3ntvGI3gkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcsUHPBHxpxHxqub9N0TEE/ZxncdExOdKr0VS/9k5krpk50jqmr0jaSZLutxYZv5Jl9uTtLDZOZK6ZOdI6pq9I2m6IkfwRMTrIuK7EfGvwH2mXf6+iDipef8pEXFFRHwVeHaJdUhaGOwcSV2ycyR1zd6RNIzWBzwR8SDg+cADGBTLg/dxnRXAu4BnAo8CDh4i99SIOD8izr/muql2Fy2pWqU6p/m623tn284t7S1aUrW66hz3dSTt0sXvV9u3bWp30ZLGosQRPI8CPp2ZmzPzZuAz+7jOkcCPMvP7mZnAh+YKzcwzM3NjZm5cf9DilpcsqWJFOgd2751li1a2uGRJFeukc9zXkTRN8d+vli5b3fKSJY1DqZMsZ0vXkaRh2DmSumTnSOqavSNpTiUGPOcCJ0bEyojYj8Fhgnu6ArhHRBzRfPxbBdYhaWGwcyR1yc6R1DV7R9JQWn8Vrcy8MCI+ClwMXAV8ZR/XuS0iTgX+OSKuBb4KHN32WiT1n50jqUt2jqSu2TuShlXkZdIz843AG/dx+SnT3v8Cg+eKStK82DmSumTnSOqavSNpGKXOwSNJkiRJkqSOOOCRJEmSJEmqnAMeSZIkSZKkyjngkSRJkiRJqpwDHkmSJEmSpMo54JEkSZIkSapckZdJL+3WTL52285yG1i2tFz29h3lsoHYf7+i+TtXryyWHb+8rlg2wNVP/LWi+Xf+5BVF8zOzWPYXbz2qWPbAzwvnl7f9gGX89DkbiuUf/DfnFcsuLsr+rWD9pWuLZU9dc02xbIB//tnFRfOffMhxRfOnCmb/4vSHF8ve9uFlxbK7cun167nXh15WLP9eB323WHbpfZFcXLZzYsvWYtk7b7ypWDbA9//0mKL5937j5UXz48ADimW//B+OL5b94xveWiy7Kwcfdh2vescHi+W//d73K5adWfD3Qjo4ImLx4mLRq84v+7vnDc9/WNH8g979b0XzKfj71ZMvu7lYNsA5MzykPIJHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKjf2AU9EnBIRh4x7HZIWBjtHUtfsHUldsnOkhWvsAx7gFMACktSVU7BzJHXrFOwdSd05BTtHWpBaHfBExIaI+E5EvCsivh0RX4qIlc3njouIf4uISyLi0xFxQEScBGwEPhwRF++6riQNw86R1DV7R1KX7BxJoyhxBM+9gP+ZmfcDbgSe01z+AeAPMvMY4FLg9Zn5CeB84OTMPC4zt8wUGhGnRsT5EXH+jdftLLBsSZUq0jmwe+/s2LKp3HcgqTbF93V23mrnSLpd8c656fqpst+BpE6UGPD8KDMvbt6/ANgQEWuBdZl5TnP5+4FHjxKamWdm5sbM3LjuoEl4ZpmkCVGkc2D33lmycnU7q5XUB8X3dRatsXMk3a5456w9cHF7q5U0NiUmJVunvT8FLCmwDUnaxc6R1DV7R1KX7BxJQ+nkUJjMvAm4ISIe1Vz0AmDXtPkWYL8u1iFpYbBzJHXN3pHUJTtH0r50Of39beBvI2IV8EPgRc3l72su3wIcP9c5MSRpSHaOpK7ZO5K6ZOdI2k2rA57MvBI4etrHfzXt/YuBh+3jaz4JfLLNdUhaGOwcSV2zdyR1yc6RNArPVixJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVLkuXya9NUvZycGLNxfLn/rZL4tlF7coysavWV0se+dNNxfLBtj8lFuK5k+9p+z62TlVLPpey39RLLsvlly9ibu849+L5e986NFzX+kOuuHIVcWyAVZdU+6+CbBkc7n8TXc9olg2wNMfeJei+ZufvaFo/vaV5f5POfjt5R5PV01tKpbdlfsc8As+9/y3FMt/yWseWSyb664vl92FKLgvlVkuG7jP3/ysaP6OG28qmk/B/P960jnFsv/4f1V+n28sptz9c9HqcvsiuW1bsewuLNpvTbHsnYUfs2svv7Fofi5bVjZ/+46i+ePgETySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUubEPeCLi9IhYNe51SFoY7BxJXbN3JHXJzpEWrrEPeIDTAQtIUldOx86R1K3TsXckded07BxpQZpzwBMRL4yISyLiWxHxweayu0fEWc3lZ0XEYc3l74uIk6Z97a3N28dExJcj4hMRcUVEfDgGTgMOAc6OiLPLfIuSamLnSOqavSOpS3aOpFJmHfBExP2A1wGPy8xjgVc0n3oH8IHMPAb4MPD2Ibb1AAbT5KOAw4FHZObbgZ8Bj83Mx86xllMj4vyIOP/663cOsTlJtZmkzmnWc3vvbGfryN+PpMk3Sb3jvo7Uf5PaOTddP3WHvh9Jk2WuI3geB3wiM68FyMzrm8uPB/5X8/4HgUcOsa3zMvMnmbkTuBjYMMpCM/PMzNyYmRsPPHASnlkmqYCJ6Zxm+7f3zlKWj/rlkuowMb3jvo60IExk56w9cPEoXyppQs219xBADpGz6zo7dmVGRADLpl1n+p+/p4AlQ65R0sJh50jqmr0jqUt2jqRi5hrwnAX8ZkQcBBARBzaXfx14fvP+ycBXm/evBB7UvP/rwNIh1nALsN+Q65XUb3aOpK7ZO5K6ZOdIKmbWAU9mfht4I3BORHwL+O/Np04DXhQRlwAv4FfPHX0XcEJEnAc8FNg0xBrOBD7vScAk2TmSumbvSOqSnSOppDkP48vM9wPv3+OyKxk8f3TP6/4SeNi0i17bXP5l4MvTrvf7094/AzhjpFVL6i07R1LX7B1JXbJzJJXiGfwkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyc75M+iT63g0H8+SPvbJY/t0fva1Y9rJrNxfLBth+4Mqi+VvXLS2WvfrKW4tlA9z20zVF85dsuFvRfHbuLBZ92sePL5Y98F8K55e3457LufZ/HFEs/y4vuLJY9p1/tKJYNgDLlxWN33nAfsWyl1/0w2LZAHnX9UXz9zv7e0Xzc+vWYtk/+ND9i2Vv/aOvFMvuyuII1i2qcjetfpnjXsEdV/PaC7sty+3D7swolt2VtYuSp6wq1/lvvbXgfn7l9/upbeV+9yx92yzaPlU0f2fB/ZDSjlrx07Fs1yN4JEmSJEmSKueAR5IkSZIkqXIOeCRJkiRJkirngEeSJEmSJKlyDngkSZIkSZIq54BHkiRJkiSpcg54JEmSJEmSKueAR5IkSZIkqXITMeCJiK83bzdExP817vVI6jc7R1KX7BxJXbN3pIVpIgY8mfnw5t0NgAUkqSg7R1KX7BxJXbN3pIVpIgY8EXFr8+5fAo+KiIsj4pXjXJOk/rJzJHXJzpHUNXtHWpiWjHsBe/hD4FWZ+Yw9PxERpwKnAiw+4ICu1yWpn2bsHNi9d5au37/LdUnqp6E7526HLu5yXZL6a6jfrw47dNJ+LZR0R0zEETzDyMwzM3NjZm5cvHr1uJcjaQGY3jtL1q4a93Ik9dz0zjnooGp20SRVanrnrD/IobLUB+49SJIkSZIkVW7SBjy3APuNexGSFgw7R1KX7BxJXbN3pAVk0gY8lwA7IuJbngRMUgfsHEldsnMkdc3ekRaQiTibVmauad5uBx4/5uVI6jk7R1KX7BxJXbN3pIVp0o7gkSRJkiRJ0ogc8EiSJEmSJFXOAY8kSZIkSVLlHPBIkiRJkiRVzgGPJEmSJElS5RzwSJIkSZIkVS4yc9xrGFlEXANcNcKX3Am4ttByas937f3Mn7S13z0z15daTBcmrHcm7ec7Kdm159e89tL5ds7cJun2X0j5Na+9dP5CWrud075J+vkupPya1146f9LWvs/eqXLAM6qIOD8zN5rfbXbp/JrXXjq/5rX3hT/f7rNrz6957aXz7Zy51Xz715xf89pL57v2fvPn28/8mtdeOr+WtfsULUmSJEmSpMo54JEkSZIkSarcQhnwnGn+WLJL59e89tL5Na+9L/z5dp9de37Nay+db+fMrebbv+b8mtdeOt+195s/337m17z20vlVrH1BnINH3YiIWzNzzbSPTwE2Zubvt5D9ZeBVmXn+Hpe/DzgBuKm56JTMvHi+25M0+cbUOQH8N+C5wBTwzsx8+3y3J2nyjalzvgLs13x4Z+C8zPyN+W5PUh3G1DuPB97C4GCQWxn8fvWD+W5P3Vgy7gVILXh1Zn5i3IuQtCCcAtwNODIzd0bEnce8Hkk9lpmP2vV+RHwS+KcxLkfSwvBO4Ncz8zsR8XLgjxns/6gCC+UpWhqziFgfEZ+MiG82/x7RXP6QiPh6RFzUvL1Pc/nKiPiHiLgkIj4KrBzrNyCpKgU752XAGzJzJ0BmXt3JNyRpopXez4mI/YDHAf9Y+nuRVIeCvZPA/s37a4GfFf9m1BqP4FGbVkbExdM+PhD4TPP+24C3ZuZXI+Iw4IvAfYErgEdn5o6IeALw58BzGPwStTkzj4mIY4ALZ9nuGyPiT4CzgD/MzK2tfleSJtU4OucI4HkRcSJwDXBaZn6/7W9M0kQa134OwInAWZl5c3vfjqQKjKN3Xgr874jYAtwMPKztb0rlOOBRm7Zk5nG7Ptj1HNHmwycARw1OXwHA/s1fo9YC74+IezGYFi9tPv9o4O0AmXlJRFwywzZfC/wCWMbgxFR/ALyhpe9H0mQbR+csB27LzI0R8WzgPcCjZriupH4ZR+fs8lvA37fwPUiqyzh655XA0zLz3yPi1cB/ZzD0UQUc8Kgri4DjM3PL9Asj4gzg7Mw8MSI2AF+e9uk5zwCemT9v3t0aEe8FXtXOciVVrkjnAD8BPtm8/2ngvfNfqqQeKNU5RMRBwEMYHMUjSbu03jsRsR44NjP/vbnoo8AXWluxivMcPOrKl4Dbz/YeEcc1764Fftq8f8q0658LnNxc92jgmH2FRsRdm7cB/AZwWXtLllSxIp3D4PwXj2vePwH4XhuLlVS9Up0Dg1ft+1xm3tbSWiX1Q4neuQFYGxH3bj5+IvCd1las4hzwqCunARubk3pdDvxuc/mbgb+IiK8Bi6dd/53AmubQwdcA582Q++GIuBS4FLgTg5cvlqRSnfOXwHOa3vkLPGRZ0kCpzgF4PvCRAmuWVLfWeyczdwD/GfhkRHwLeAHw6oLfg1oWmUMdHSpJkiRJkqQJ5RE8kiRJkiRJlXPAI0mSJEmSVDkHPJIkSZIkSZVzwCNJkiRJklQ5BzySJEmSJEmVc8AjSZIkSZJUOQc8kiRJkiRJlfv/AbrK5WyWgxLgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "inp_sentence = \"i was told ten thousand in each pack\"\n",
        "reply(inp_sentence, transformer,  tokenizer_q, tokenizer_a, \"decoder_layer2_block2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3db5f71",
      "metadata": {
        "id": "b3db5f71"
      },
      "source": [
        "## OUTPUTS are not absurd at all, It has LEARNED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298cfd21",
      "metadata": {
        "id": "298cfd21",
        "outputId": "05bc99e6-6ed3-4740-c1bf-5ceb5827080c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Attention_Blocks: ['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2']\n",
            "Input: i did not sleep well\n",
            "Predicted translation: great bye\n"
          ]
        }
      ],
      "source": [
        "inp_sentence = \"i did not sleep well\"\n",
        "reply(inp_sentence, transformer,  tokenizer_q, tokenizer_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd920ba",
      "metadata": {
        "scrolled": true,
        "id": "afd920ba",
        "outputId": "abe500dd-8fd2-4bc8-96ce-44fee05a36a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>84608</th>\n",
              "      <td>irwin professional journalism time now go back...</td>\n",
              "      <td>i will frank i will something came up okay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6889</th>\n",
              "      <td>you mean with you and that woman chained to ya...</td>\n",
              "      <td>you treat folks special when they company it i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125210</th>\n",
              "      <td>why has he bothered you before</td>\n",
              "      <td>is it news to you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24800</th>\n",
              "      <td>i was told ten thousand in each pack</td>\n",
              "      <td>you did not count it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76507</th>\n",
              "      <td>he just wants me to make him cinnamon cookies ...</td>\n",
              "      <td>i think he wants more than your cookies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81602</th>\n",
              "      <td>no no  you boys are tired</td>\n",
              "      <td>no we are not  jack</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 question  \\\n",
              "84608   irwin professional journalism time now go back...   \n",
              "6889    you mean with you and that woman chained to ya...   \n",
              "125210                     why has he bothered you before   \n",
              "24800                i was told ten thousand in each pack   \n",
              "76507   he just wants me to make him cinnamon cookies ...   \n",
              "81602                           no no  you boys are tired   \n",
              "\n",
              "                                                   answer  \n",
              "84608          i will frank i will something came up okay  \n",
              "6889    you treat folks special when they company it i...  \n",
              "125210                                  is it news to you  \n",
              "24800                                you did not count it  \n",
              "76507             i think he wants more than your cookies  \n",
              "81602                                 no we are not  jack  "
            ]
          },
          "execution_count": 1075,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.iloc[400:406]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14569967",
      "metadata": {
        "id": "14569967",
        "outputId": "11586bb2-2376-48f8-a90a-5410411072ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23940</th>\n",
              "      <td>south america</td>\n",
              "      <td>we leave miami in an hour soon is we get some ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54489</th>\n",
              "      <td>i did not sleep well</td>\n",
              "      <td>do you want to talk about it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>really both of you why not</td>\n",
              "      <td>just because</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38692</th>\n",
              "      <td>what department store did they go to</td>\n",
              "      <td>mcintire is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21519</th>\n",
              "      <td>i should not say this but you are pretty gabri...</td>\n",
              "      <td>really i always think myself so ugly no not ug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116659</th>\n",
              "      <td>i do not know</td>\n",
              "      <td>do not know what</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 question  \\\n",
              "23940                                       south america   \n",
              "54489                                i did not sleep well   \n",
              "15996                          really both of you why not   \n",
              "38692                what department store did they go to   \n",
              "21519   i should not say this but you are pretty gabri...   \n",
              "116659                                      i do not know   \n",
              "\n",
              "                                                   answer  \n",
              "23940   we leave miami in an hour soon is we get some ...  \n",
              "54489                        do you want to talk about it  \n",
              "15996                                        just because  \n",
              "38692                                         mcintire is  \n",
              "21519   really i always think myself so ugly no not ug...  \n",
              "116659                                   do not know what  "
            ]
          },
          "execution_count": 1070,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation.iloc[400:406]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a112d4ac",
      "metadata": {
        "scrolled": true,
        "id": "a112d4ac",
        "outputId": "02e448c1-8912-4721-e7b2-a2f96db89c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: and the fifty is all gone huh who is the ten for\n",
            "Predicted translation: the chance is impossible\n",
            "Actual: the websters\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: rocky do you have any representation a manager\n",
            "Predicted translation: no  just me\n",
            "Actual: no  just me\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: jeanne let me introduce the king is half brother the dogged lord dunois\n",
            "Predicted translation: then lord have given my lord is name in rome\n",
            "Actual: then lord dunois show me the way to the other side of the river\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: father martineau but i do not see him as a candidate\n",
            "Predicted translation: could there have been anyone else\n",
            "Actual: could there have been anyone else\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: i have not read you your rights\n",
            "Predicted translation: would you mind saying that into your bag\n",
            "Actual: would you mind saying that into your bag\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: is this a good spot\n",
            "Predicted translation: i am not sure look at it  time is it  time to look at it\n",
            "Actual: i am not burying him here\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: my turn what is your favorite song\n",
            "Predicted translation: same time when you were at school\n",
            "Actual: soft and wet by the artist formerly known as prince\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: why do not you dump it mail it off give the fucking fbi a present\n",
            "Predicted translation: why do not you dump the fat lady\n",
            "Actual: why do not you dump the fat lady\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: time to impact\n",
            "Predicted translation: twelve seconds\n",
            "Actual: twelve seconds\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: run for your lives boys  it is that great twogun dogcatcher from kansas\n",
            "Predicted translation: you you seen all the in a to years old man\n",
            "Actual: mcmasters is not it  listen you seen a black stallion with\n"
          ]
        }
      ],
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from tqdm import tqdm\n",
        "test_q = train[\"question\"].values[:10]\n",
        "test_a = train[\"answer\"].values[:10]\n",
        "bss = []\n",
        "for i in range(10):\n",
        "    input_test_sentence = test_q[i]\n",
        "    input_sentence, pred_string = reply(input_test_sentence, transformer,  tokenizer_q, tokenizer_a, plot='')\n",
        "    print(\"Actual:\", test_a[i])\n",
        "    reference = [test_a[i].split()] # the original\n",
        "    translation = pred_string.split() # trasilated using model\n",
        "#     bs = bleu.sentence_bleu(reference, translation)\n",
        "#     bss.append(bs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86d5e23",
      "metadata": {
        "id": "c86d5e23",
        "outputId": "17b96b38-4b9a-4cc9-995c-6daf5ebe1631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: you have been talking to her on the phone for weeks\n",
            "Predicted translation: yes i can imagine that she was there that i am\n",
            "Actual: it was only a few times\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: into our group\n",
            "Predicted translation: leave me at the house\n",
            "Actual: it is really hard to do some kids try for all ofhigh school and never make it\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: we will do that but how are we going to hold him he can change himself into a man he can disappear\n",
            "Predicted translation: where should we be  we still have to move a rock n roll\n",
            "Actual: that is the chance we have to take\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: oh nothing wade how ya doin there\n",
            "Predicted translation: holy cow\n",
            "Actual: stan grossman looked at your proposal says it is pretty sweet\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: i do not understand \n",
            "Predicted translation: i am so scared i did not take you back\n",
            "Actual: did you say rape her\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: you are missing the game for us\n",
            "Predicted translation: you are the one that can not be done\n",
            "Actual: no  i am missing the game for you\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: i took sleeping pills\n",
            "Predicted translation: i should have taken that great life to see your father and him has gone to hell out of there\n",
            "Actual: do you know where you are fran\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: this is all cause of your mom kyle she is such a bitch  agh i mean  she is such a meanie\n",
            "Predicted translation: i know her there would be more to her again  i never met her i am not sure\n",
            "Actual: and she is getting worse\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: the insurance business\n",
            "Predicted translation: jesus christ an old man got that woman in my life\n",
            "Actual: it is a good honest business is not it\n",
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: have you evacuated anyone\n",
            "Predicted translation: or what about us\n",
            "Actual: only that floor\n"
          ]
        }
      ],
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "from tqdm import tqdm\n",
        "test_q = validation[\"question\"].values[100:110]\n",
        "test_a = validation[\"answer\"].values[100:110]\n",
        "bss = []\n",
        "for i in range(10):\n",
        "    input_test_sentence = test_q[i]\n",
        "    input_sentence, pred_string = reply(input_test_sentence, load_transformer,  tokenizer_q, tokenizer_a, plot='')\n",
        "    print(\"Actual:\", test_a[i])\n",
        "    reference = [test_a[i].split()] # the original\n",
        "    translation = pred_string.split() # trasilated using model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5221356",
      "metadata": {
        "id": "f5221356",
        "outputId": "792f597a-fc4f-4d16-bf92-0ade516a6bba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x27e0fd93940>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# num_layers = 2\n",
        "# d_model = 256\n",
        "# dff = 512\n",
        "# num_heads = 8\n",
        "# input_vocab_size = tokenizer_q.vocab_size + 2\n",
        "# target_vocab_size = tokenizer_a.vocab_size + 2\n",
        "# dropout_rate = 0.1\n",
        "\n",
        "# load_transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "#                           input_vocab_size, target_vocab_size, \n",
        "#                           pe_input=input_vocab_size, \n",
        "#                           pe_target=target_vocab_size,\n",
        "#                           rate=dropout_rate)\n",
        "# load_transformer.load_weights('transformer_model/weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2d6845",
      "metadata": {
        "id": "5d2d6845",
        "outputId": "3795df2c-1998-4e0d-e9ec-eb947161b025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: i was told ten thousand in each pack\n",
            "Predicted translation: you did not count it\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('i was told ten thousand in each pack', 'you did not count it')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"i was told ten thousand in each pack\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a, \"decoder_layer2_block2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76541209",
      "metadata": {
        "id": "76541209"
      },
      "source": [
        "* Results are great, better than encoder-decoder with bahadenau attention mechanism\n",
        "* Still we can see the results are not perfect, because the architecture has less parameters plus the dataset is not very big and transformers works close to humans with large data and large trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144f2d50",
      "metadata": {
        "id": "144f2d50"
      },
      "source": [
        "# On random inputs!\n",
        "### Results are genuine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c357afa8",
      "metadata": {
        "id": "c357afa8",
        "outputId": "5fef7bf4-6906-4686-85e8-6d38a9b478aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: hi\n",
            "Predicted translation: hi\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('hi', 'hi')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"hi\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8dbf4e6",
      "metadata": {
        "id": "a8dbf4e6",
        "outputId": "7713ee76-ff43-4133-f4db-992a662c7e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: Where have you been\n",
            "Predicted translation: i think you came up with this town again\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Where have you been', 'i think you came up with this town again')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"Where have you been\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b357781a",
      "metadata": {
        "id": "b357781a"
      },
      "source": [
        "#### Making sense haha : ) \"who are you\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d86b1c",
      "metadata": {
        "id": "b1d86b1c",
        "outputId": "3c440aca-9759-467d-d204-e81d8902f0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: who are you\n",
            "Predicted translation: i am the guy who is he\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('who are you', 'i am the guy who is he')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"who are you\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f0fab3",
      "metadata": {
        "id": "58f0fab3"
      },
      "source": [
        "####  Again making sense \"where do you live\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635f4b9c",
      "metadata": {
        "id": "635f4b9c",
        "outputId": "bd6d37bf-46db-4a79-91f3-3a130186493e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: where do you live\n",
            "Predicted translation: in new york\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('where do you live', 'in new york')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"where do you live\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ecdc49",
      "metadata": {
        "id": "88ecdc49",
        "outputId": "f2181b94-e29b-46ac-ac2b-f0cdbb9a23b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: what is your name\n",
            "Predicted translation: my name is sir te i am not the one i am talking about you\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('what is your name',\n",
              " 'my name is sir te i am not the one i am talking about you')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"what is your name\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f480ff",
      "metadata": {
        "scrolled": true,
        "id": "b5f480ff",
        "outputId": "08c4d18f-fd00-419c-adbe-6444b7c0946e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============\n",
            "Got end token\n",
            "=============\n",
            "Input: why are you angry with me is there anything i did wrong\n",
            "Predicted translation: i did not know you had something to do for me\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('why are you angry with me is there anything i did wrong',\n",
              " 'i did not know you had something to do for me')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp_sentence = \"why are you angry with me is there anything i did wrong\"\n",
        "reply(inp_sentence, load_transformer,  tokenizer_q, tokenizer_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57d9937",
      "metadata": {
        "id": "e57d9937"
      },
      "source": [
        "# Even with this small architecture its working fine, what else it could do If I train it with trainable parameters with more data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}